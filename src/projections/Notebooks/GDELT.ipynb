{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sys\n",
    "import yaml\n",
    "import gc\n",
    "\n",
    "from zipfile import ZipFile, ZIP_DEFLATED\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "from glob import glob, iglob\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "\n",
    "import shapefile\n",
    "import shapely\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time(file):\n",
    "    t = file.split('.')[0]\n",
    "    \n",
    "    if len(t) == 8:\n",
    "        t = datetime.strptime(t, '%Y%m%d') \n",
    "    elif len(t) == 6:\n",
    "        t = datetime.strptime(t, '%Y%m')\n",
    "    else:\n",
    "        t = datetime.strptime(t, '%Y')\n",
    "        \n",
    "    return t\n",
    "    \n",
    "    \n",
    "def download(url, name):\n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    with open(name, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "    return True\n",
    "\n",
    "\n",
    "def get_current_batch_dict():\n",
    "    c_files = [Path(x).name.replace('.feather', '') for x in glob(str(save_folder / '*.feather'))]\n",
    "    batches = {}\n",
    "    for file in c_files:\n",
    "        date, batch = file.split('_')\n",
    "        batches[date] = max(batches.get(date, 0), int(batch))\n",
    "    return batches\n",
    "\n",
    "\n",
    "def load_processed(file):\n",
    "    try:\n",
    "        with open(file, 'r') as f:\n",
    "            processed = yaml.safe_load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(file, 'not found')\n",
    "        processed = []\n",
    "    return processed\n",
    "\n",
    "\n",
    "def save_processed(content, file):\n",
    "    with open(file, 'w') as f:\n",
    "        yaml.safe_dump(content, f)\n",
    "        \n",
    "        \n",
    "def match_geo(locs, lon_col, lat_col, country_col, use_pool=True):\n",
    "    f = partial(match_single_geo, lon_col=lon_col, lat_col=lat_col, country_col=country_col)\n",
    "    \n",
    "    if use_pool:\n",
    "        with Pool(30) as p:\n",
    "            results = p.map(f, locs.iterrows())\n",
    "    else:\n",
    "        results = [f(x) for x in locs.iterrows()]\n",
    "    \n",
    "    for result in results:\n",
    "        idx = result['idx']\n",
    "        locs.loc[idx, 'adm2'] = result.get('adm2')\n",
    "        locs.loc[idx, 'adm1'] = result.get('adm1')\n",
    "        locs.loc[idx, 'adm0'] = result.get('adm0')\n",
    "        locs.loc[idx, 'nearest_loc'] = result['nearest_loc']\n",
    "        \n",
    "    return locs\n",
    "\n",
    "\n",
    "def country_shapes(country):\n",
    "    if country in shapes_by_country:\n",
    "        yield from shapes_by_country[country]\n",
    "    else:\n",
    "        for shapes in shapes_by_country.values():\n",
    "            yield from shapes\n",
    "        \n",
    "        \n",
    "def match_single_geo(row, lon_col, lat_col, country_col):\n",
    "    if isinstance(row, tuple):\n",
    "        row = row[1]\n",
    "        \n",
    "    if isinstance(row['adm1'], str):\n",
    "        return {'nearest_loc': row['nearest_loc'], \n",
    "                'adm1': row['adm1'],\n",
    "                'adm2': row['adm2'], \n",
    "                'adm0': row['adm0'],\n",
    "                'idx': row.name}\n",
    "    \n",
    "    results = {'nearest_loc': False, 'idx': row.name}\n",
    "\n",
    "    point = Point((float(row[lon_col]), float(row[lat_col])))\n",
    "    nearest_record = None\n",
    "    nearest_distance = np.inf\n",
    "    for shape, country, adm1, adm2 in country_shapes(row[country_col]):\n",
    "        if point.within(shape):\n",
    "            results['adm0'] = country\n",
    "            results['adm2'] = adm2\n",
    "            results['adm1'] = adm1\n",
    "            break\n",
    "    else:\n",
    "        for shape, country, adm1, adm2 in country_shapes(row[country_col]):\n",
    "            distance = point.distance(shape)\n",
    "            if distance < nearest_distance:\n",
    "                nearest_distance = distance\n",
    "                nearest_record = (country, adm1, adm2)\n",
    "\n",
    "        if nearest_record:\n",
    "            results['nearest_loc'] = True\n",
    "            results['adm0'] = nearest_record[0]\n",
    "            results['adm1'] = nearest_record[1]\n",
    "            results['adm2'] = nearest_record[2]\n",
    "    \n",
    "    return results    \n",
    "\n",
    "\n",
    "def get_locs(df, cols):\n",
    "    locs = df.loc[df[cols].notnull().all(axis=1), cols].drop_duplicates()\n",
    "    locs['adm1'] = np.nan\n",
    "    locs['adm2'] = np.nan\n",
    "    locs['adm0'] = np.nan\n",
    "    locs['nearest_loc'] = False\n",
    "    return locs\n",
    "\n",
    "\n",
    "def append_adm_codes(df, lat_col, lon_col, country_col, geo_type_col, use_pool=True):\n",
    "    # Get locs\n",
    "    on_cols = [lat_col, lon_col, country_col]\n",
    "    locs = get_locs(df, on_cols)\n",
    "\n",
    "    # Append adm codes\n",
    "    locs = match_geo(locs, lon_col, lat_col, country_col, use_pool=use_pool)\n",
    "\n",
    "    # Merge them back\n",
    "    df = df.merge(locs, on=on_cols, how='left')\n",
    "    \n",
    "    df['adm0'].fillna(df[country_col], inplace=True)\n",
    "    \n",
    "    # Handle geo precision\n",
    "    null_mask = df[geo_type_col].isnull()\n",
    "    no_adm1_mask = df[geo_type_col] <= 1\n",
    "    no_adm2_mask = np.logical_or(df[geo_type_col] <= 2, df[geo_type_col] >= 5)\n",
    "    df.loc[no_adm1_mask, 'adm1'] = np.nan\n",
    "    df.loc[no_adm2_mask, 'adm2'] = np.nan\n",
    "    df.loc[null_mask, ['adm1', 'adm2']] = np.nan\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def convert_cols_to_numeric(df, cols):\n",
    "    for col in cols:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Output/GDELT/processed.yml not found\n"
     ]
    }
   ],
   "source": [
    "# Constants for later\n",
    "keep_cols = ['SQLDATE', 'ActionGeo_Lat', 'ActionGeo_Long', 'ActionGeo_Type', 'ActionGeo_CountryCode', 'EventCode',\n",
    "             'NumMentions', 'IsRootEvent', 'NumSources', 'NumArticles', 'AvgTone']\n",
    "download_folder = Path('../Downloads')\n",
    "save_folder = Path('../Output/GDELT/')\n",
    "collapse_folder = Path('../Output/GDELT_collapsed/')\n",
    "batch_size = 50\n",
    "downloaded = [Path(x).name for x in glob(f'../Downloads/*.zip')]\n",
    "\n",
    "processed_file = save_folder / 'processed.yml'\n",
    "processed = load_processed(processed_file)\n",
    "\n",
    "lat_col = 'ActionGeo_Lat'\n",
    "lon_col = 'ActionGeo_Long'\n",
    "geo_type_col = 'ActionGeo_Type'\n",
    "country_col = 'ActionGeo_CountryCode'\n",
    "group_cols = ['EventCode']\n",
    "dist_cols = ['NumMentions', 'IsRootEvent', 'NumSources', 'NumArticles', 'AvgTone']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdelt = 'http://data.gdeltproject.org/events/'\n",
    "response = requests.get(gdelt + 'index.html')\n",
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current header row\n",
    "header = requests.get('https://www.gdeltproject.org/data/lookups/CSV.header.dailyupdates.txt')\n",
    "header = header.text.strip().split('\\t')\n",
    "# Header row up until march 2013\n",
    "ignore_cols = {'SOURCEURL'}\n",
    "header_old = [x for x in header if x not in ignore_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shape_tuple(shape_record_tuple, i):\n",
    "    shape, record = shape_record_tuple\n",
    "    country = record[0]\n",
    "    shape = shapely.geometry.shape(shape)  \n",
    "\n",
    "    adm1, adm2 = None, None\n",
    "    if i >= 1:\n",
    "        adm1 = record[2]\n",
    "    if i == 2:\n",
    "        adm2 = record[5]\n",
    "        \n",
    "    return (shape, country, adm1, adm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 100%|██████████| 256/256 [05:21<00:00,  1.26s/it]\n",
      "1: 100%|██████████| 3610/3610 [05:50<00:00, 10.30it/s]  \n",
      "2: 100%|██████████| 45962/45962 [04:25<00:00, 173.15it/s] \n"
     ]
    }
   ],
   "source": [
    "shps = glob('../Shapefiles/preprocessed/*.shp')\n",
    "shapes_by_country = {}\n",
    "for i, shp in tqdm(enumerate(shps), total=len(shps)):\n",
    "    codes = Path(shp).name.replace('.shp', '').split('_')\n",
    "    shp = shapefile.Reader(shp)\n",
    "    shapes = [shapely.geometry.shape(s) for s in shp.shapes()]\n",
    "    records = shp.records()\n",
    "    \n",
    "    for code in codes:\n",
    "        shapes_by_country[code] = []\n",
    "        for shape, record in zip(shapes, records):\n",
    "            shapes_by_country[code].append((shape, *record))\n",
    "    \n",
    "assert 'GT' in shapes_by_country and 'PM' in shapes_by_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping file <a href=\"md5sums\">md5sums</a>\n",
      "Skipping file <a href=\"filesizes\">filesizes</a>\n",
      "Skipping file <a href=\"GDELT.MASTERREDUCEDV2.1979-2013.zip\">GDELT.MASTERREDUCEDV2.1979-2013.zip</a>\n",
      "Skipping file <a href=\"GDELT.MASTERREDUCEDV2.1979-2013.zip\">GDELT.MASTERREDUCEDV2.1979-2013.zip</a>\n",
      "Total years: 42\n",
      "Total files: 3068\n"
     ]
    }
   ],
   "source": [
    "all_a = soup.find_all('a')\n",
    "all_files = {}\n",
    "\n",
    "header_cutoff_date = datetime.strptime('201303', '%Y%m')\n",
    "for a in all_a:\n",
    "    name = a.text\n",
    "    try:\n",
    "        t = get_time(name)\n",
    "    except ValueError:\n",
    "        print(f'Skipping file {a}')\n",
    "        continue\n",
    "        \n",
    "    if t.year >= 2021:\n",
    "        continue\n",
    "        \n",
    "    if t.year not in all_files:\n",
    "        all_files[t.year] = []\n",
    "        \n",
    "    all_files[t.year].append({\n",
    "        'date': t,\n",
    "        'name': name,\n",
    "        'href': a['href'],\n",
    "        'header': header if t > header_cutoff_date else header_old,\n",
    "        'append': set() if t > header_cutoff_date else ignore_cols\n",
    "    })\n",
    "    \n",
    "print('Total years:', len(all_files))\n",
    "print('Total files:', len(all_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_combine(elements, batch_size, batch, download_folder):\n",
    "    dfs = []\n",
    "    for element in tqdm(elements[batch_size * (batch - 1): batch_size * batch], \n",
    "                        desc=f'Year {year}. Batch {batch} of {n_batches}'): \n",
    "        zfile = download_folder / element['name']\n",
    "\n",
    "        if element['name'] not in downloaded:\n",
    "            download(gdelt + element['href'], zfile)\n",
    "\n",
    "        with ZipFile(zfile, 'r') as f:\n",
    "            files = f.namelist()\n",
    "            f.extractall(download_folder)\n",
    "\n",
    "        for file in files:\n",
    "            df = pd.read_csv(download_folder / files[0], sep='\\t', names=element['header'], dtype=str)\n",
    "            for col in element['append']:\n",
    "                df[col] = np.nan\n",
    "            dfs.append(df[keep_cols])\n",
    "            os.remove(download_folder / file)       \n",
    "\n",
    "    # Combine in a single df\n",
    "    print('Combining...')\n",
    "    df = dfs[0].append(dfs[1:], ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2020. Batch 1 of 8: 100%|██████████| 50/50 [01:10<00:00,  1.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2020. Batch 2 of 8: 100%|██████████| 50/50 [01:10<00:00,  1.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2020. Batch 3 of 8: 100%|██████████| 50/50 [01:00<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2020. Batch 4 of 8: 100%|██████████| 50/50 [01:05<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2020. Batch 5 of 8: 100%|██████████| 50/50 [01:03<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2020. Batch 6 of 8: 100%|██████████| 50/50 [00:57<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2020. Batch 7 of 8: 100%|██████████| 50/50 [00:45<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2020. Batch 8 of 8: 100%|██████████| 16/16 [00:15<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2019. Batch 1 of 8: 100%|██████████| 50/50 [01:14<00:00,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2019. Batch 2 of 8: 100%|██████████| 50/50 [01:24<00:00,  1.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2019. Batch 3 of 8: 100%|██████████| 50/50 [01:21<00:00,  1.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2019. Batch 4 of 8: 100%|██████████| 50/50 [01:16<00:00,  1.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2019. Batch 5 of 8: 100%|██████████| 50/50 [01:12<00:00,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2019. Batch 6 of 8: 100%|██████████| 50/50 [01:17<00:00,  1.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2019. Batch 7 of 8: 100%|██████████| 50/50 [01:12<00:00,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2019. Batch 8 of 8: 100%|██████████| 15/15 [00:17<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2018. Batch 1 of 8: 100%|██████████| 50/50 [01:20<00:00,  1.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2018. Batch 2 of 8: 100%|██████████| 50/50 [01:26<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2018. Batch 3 of 8: 100%|██████████| 50/50 [01:27<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2018. Batch 4 of 8: 100%|██████████| 50/50 [01:28<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2018. Batch 5 of 8: 100%|██████████| 50/50 [01:21<00:00,  1.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2018. Batch 6 of 8: 100%|██████████| 50/50 [01:27<00:00,  1.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2018. Batch 7 of 8: 100%|██████████| 50/50 [01:22<00:00,  1.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2018. Batch 8 of 8: 100%|██████████| 15/15 [00:18<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2017. Batch 1 of 8: 100%|██████████| 50/50 [01:40<00:00,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2017. Batch 2 of 8: 100%|██████████| 50/50 [01:39<00:00,  1.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2017. Batch 3 of 8: 100%|██████████| 50/50 [01:33<00:00,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2017. Batch 4 of 8: 100%|██████████| 50/50 [01:27<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2017. Batch 5 of 8: 100%|██████████| 50/50 [01:20<00:00,  1.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2017. Batch 6 of 8: 100%|██████████| 50/50 [01:28<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2017. Batch 7 of 8: 100%|██████████| 50/50 [01:30<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2017. Batch 8 of 8: 100%|██████████| 15/15 [00:19<00:00,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2016. Batch 1 of 8: 100%|██████████| 50/50 [01:44<00:00,  2.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2016. Batch 2 of 8: 100%|██████████| 50/50 [01:38<00:00,  1.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2016. Batch 3 of 8: 100%|██████████| 50/50 [01:39<00:00,  1.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2016. Batch 4 of 8: 100%|██████████| 50/50 [01:35<00:00,  1.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2016. Batch 5 of 8: 100%|██████████| 50/50 [01:33<00:00,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2016. Batch 6 of 8: 100%|██████████| 50/50 [01:40<00:00,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2016. Batch 7 of 8: 100%|██████████| 50/50 [01:32<00:00,  1.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2016. Batch 8 of 8: 100%|██████████| 16/16 [00:25<00:00,  1.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2015. Batch 1 of 8: 100%|██████████| 50/50 [01:12<00:00,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2015. Batch 2 of 8: 100%|██████████| 50/50 [01:27<00:00,  1.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2015. Batch 3 of 8: 100%|██████████| 50/50 [01:25<00:00,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2015. Batch 4 of 8: 100%|██████████| 50/50 [01:30<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2015. Batch 5 of 8: 100%|██████████| 50/50 [01:30<00:00,  1.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2015. Batch 6 of 8: 100%|██████████| 50/50 [01:33<00:00,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2015. Batch 7 of 8: 100%|██████████| 50/50 [01:33<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2015. Batch 8 of 8: 100%|██████████| 15/15 [00:27<00:00,  1.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2014. Batch 1 of 8: 100%|██████████| 50/50 [00:54<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2014. Batch 2 of 8: 100%|██████████| 50/50 [01:01<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2014. Batch 3 of 8: 100%|██████████| 50/50 [01:03<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2014. Batch 4 of 8: 100%|██████████| 50/50 [01:06<00:00,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2014. Batch 5 of 8: 100%|██████████| 50/50 [01:10<00:00,  1.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2014. Batch 6 of 8: 100%|██████████| 50/50 [01:18<00:00,  1.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2014. Batch 7 of 8: 100%|██████████| 50/50 [01:12<00:00,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2014. Batch 8 of 8: 100%|██████████| 11/11 [00:12<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2013. Batch 1 of 6: 100%|██████████| 50/50 [01:20<00:00,  1.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2013. Batch 2 of 6: 100%|██████████| 50/50 [00:33<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2013. Batch 3 of 6: 100%|██████████| 50/50 [01:01<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2013. Batch 4 of 6: 100%|██████████| 50/50 [01:09<00:00,  1.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2013. Batch 5 of 6: 100%|██████████| 50/50 [01:06<00:00,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2013. Batch 6 of 6: 100%|██████████| 28/28 [00:31<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2012. Batch 1 of 1: 100%|██████████| 12/12 [04:26<00:00, 22.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2011. Batch 1 of 1: 100%|██████████| 12/12 [04:14<00:00, 21.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2010. Batch 1 of 1: 100%|██████████| 12/12 [02:54<00:00, 14.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2009. Batch 1 of 1: 100%|██████████| 12/12 [02:59<00:00, 14.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2008. Batch 1 of 1: 100%|██████████| 12/12 [01:51<00:00,  9.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2007. Batch 1 of 1: 100%|██████████| 12/12 [01:32<00:00,  7.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2006. Batch 1 of 1: 100%|██████████| 12/12 [00:49<00:00,  4.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2005. Batch 1 of 1: 100%|██████████| 1/1 [00:27<00:00, 27.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2004. Batch 1 of 1: 100%|██████████| 1/1 [00:37<00:00, 37.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2003. Batch 1 of 1: 100%|██████████| 1/1 [00:47<00:00, 47.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2002. Batch 1 of 1: 100%|██████████| 1/1 [00:35<00:00, 35.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2001. Batch 1 of 1: 100%|██████████| 1/1 [00:45<00:00, 45.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2000. Batch 1 of 1: 100%|██████████| 1/1 [00:36<00:00, 36.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 1999. Batch 1 of 1: 100%|██████████| 1/1 [00:34<00:00, 34.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 1998. Batch 1 of 1: 100%|██████████| 1/1 [00:34<00:00, 34.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 1997. Batch 1 of 1: 100%|██████████| 1/1 [00:28<00:00, 28.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 1996. Batch 1 of 1: 100%|██████████| 1/1 [00:20<00:00, 20.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 1995. Batch 1 of 1: 100%|██████████| 1/1 [00:13<00:00, 13.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 1994. Batch 1 of 1: 100%|██████████| 1/1 [00:13<00:00, 13.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 1993. Batch 1 of 1: 100%|██████████| 1/1 [00:09<00:00,  9.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 1992. Batch 1 of 1: 100%|██████████| 1/1 [00:07<00:00,  7.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 1991. Batch 1 of 1: 100%|██████████| 1/1 [00:11<00:00, 11.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 1990. Batch 1 of 1: 100%|██████████| 1/1 [00:08<00:00,  8.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 1989. Batch 1 of 1: 100%|██████████| 1/1 [00:07<00:00,  7.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 1988. Batch 1 of 1: 100%|██████████| 1/1 [00:08<00:00,  8.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 1987. Batch 1 of 1: 100%|██████████| 1/1 [00:08<00:00,  8.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 1986. Batch 1 of 1: 100%|██████████| 1/1 [00:08<00:00,  8.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 1985. Batch 1 of 1: 100%|██████████| 1/1 [00:07<00:00,  7.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 1984. Batch 1 of 1: 100%|██████████| 1/1 [00:06<00:00,  6.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 1983. Batch 1 of 1: 100%|██████████| 1/1 [00:06<00:00,  6.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 1982. Batch 1 of 1: 100%|██████████| 1/1 [00:05<00:00,  5.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 1981. Batch 1 of 1: 100%|██████████| 1/1 [00:05<00:00,  5.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 1980. Batch 1 of 1: 100%|██████████| 1/1 [00:04<00:00,  4.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 1979. Batch 1 of 1: 100%|██████████| 1/1 [00:03<00:00,  3.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n",
      "Matching geo...\n",
      "Saving...\n"
     ]
    }
   ],
   "source": [
    "# Process files\n",
    "for year, elements in all_files.items():\n",
    "    elements = sorted(elements, key=lambda x: x['date'])\n",
    "    n_batches = len(elements) // batch_size + int(len(elements) % batch_size > 0)\n",
    "    \n",
    "    for batch in range(1, n_batches + 1):\n",
    "        batch_name = f'{year}_b{batch:02}.feather'\n",
    "        \n",
    "        # If the batch exists already, skip it\n",
    "        if batch_name in processed:\n",
    "            continue\n",
    "        \n",
    "        df = download_and_combine(elements, batch_size, batch, download_folder)        \n",
    "        \n",
    "        # Parse numerics\n",
    "        df = convert_cols_to_numeric(df, dist_cols + [geo_type_col])\n",
    "        \n",
    "        # Append ADM\n",
    "        print('Matching geo...')\n",
    "        df = append_adm_codes(df, lat_col, lon_col, country_col, geo_type_col)\n",
    "        \n",
    "        # Save by sqldate\n",
    "        print('Saving...')\n",
    "        batch_dict = get_current_batch_dict()\n",
    "        for date, subdf in df.groupby('SQLDATE'):\n",
    "            n = batch_dict.get(date, -1) + 1\n",
    "            subdf.reset_index(drop=True).to_feather(save_folder / f'{date}_{n:02}.feather')\n",
    "            \n",
    "        # Add to processed\n",
    "        processed.append(batch_name)\n",
    "        save_processed(processed, processed_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def add_missing_locations(file, cols):\n",
    "#     df = pd.read_feather(file)\n",
    "    \n",
    "#     mask = np.logical_and(df['adm1'].isnull(), df[country_col].notnull())\n",
    "#     if mask.sum() == 0:\n",
    "#         return False\n",
    "    \n",
    "#     idx = df[mask].index\n",
    "#     subdf = append_adm_codes(df.loc[mask, cols], lat_col, lon_col, country_col, geo_type_col, use_pool=False)\n",
    "#     subdf.index = idx\n",
    "#     df[mask] = subdf\n",
    "    \n",
    "#     df.to_feather(file)\n",
    "#     return True\n",
    "\n",
    "# files = glob(str(save_folder / '*.feather'))\n",
    "# df = pd.read_feather(files[0])\n",
    "# cols = [x for x in df.columns if x not in ['adm0', 'adm1', 'adm2', 'nearest_loc']]\n",
    "# del df\n",
    "\n",
    "# f = partial(add_missing_locations, cols=cols)\n",
    "# with Pool(20) as p:\n",
    "#     results = [x for x in tqdm(p.imap_unordered(f, files), total=len(files))]\n",
    "# print(sum(results), 'files changed')\n",
    "# del results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collapsed(df, group_cols, dist_cols):    \n",
    "    pivot = pd.pivot_table(df, \n",
    "                           index=group_cols, \n",
    "                           values=dist_cols,\n",
    "                           aggfunc=[np.mean, np.sum, np.median, np.std])\n",
    "    pivot.columns = ['_'.join(x[::-1]) for x in pivot.columns]\n",
    "    pivot['count'] = df.groupby(group_cols)[dist_cols[0]].count()\n",
    "    return pivot.reset_index()\n",
    "        \n",
    "        \n",
    "def increase_month(date: int):    \n",
    "    month = date % 100\n",
    "    if month == 12:\n",
    "        new_date = ((date // 100) + 1) * 100 + 1\n",
    "    else:\n",
    "        new_date = date + 1\n",
    "    return new_date\n",
    "\n",
    "\n",
    "def increase_week(date: int):    \n",
    "    week = date % 100\n",
    "    assert 0 < week < 54\n",
    "    \n",
    "    if week < 52:\n",
    "        new_date = date + 1\n",
    "    elif week == 53:\n",
    "        new_date = (date // 100 + 1) * 100 + 1\n",
    "    else:\n",
    "        year = date // 100\n",
    "        _, to_ = week_range(date)\n",
    "        if to_ == datetime(year, 12, 31):\n",
    "            new_date = (year + 1) * 100 + 1\n",
    "        else:\n",
    "            new_date = date + 1\n",
    "        \n",
    "    return new_date\n",
    "\n",
    "\n",
    "def increase_day(date: int):\n",
    "    date = datetime.strptime(str(date), '%Y%m%d')\n",
    "    date += timedelta(days=1)\n",
    "    return format_date(date)\n",
    "\n",
    "\n",
    "def format_date(date):\n",
    "    return int(datetime.strftime(date, '%Y%m%d'))\n",
    "\n",
    "\n",
    "def week_range(year_week):\n",
    "    year = year_week // 100\n",
    "    week = year_week % 100\n",
    "    assert 0 < week < 54\n",
    "    date = datetime.strptime(f'{year}01-0', '%Y%U-%w')\n",
    "    \n",
    "    if date.day == 1:\n",
    "        from_ = date\n",
    "        to_ = date + timedelta(days=6)\n",
    "    else:\n",
    "        from_ = datetime(year, 1, 1)\n",
    "        to_ = date\n",
    "    \n",
    "    if week > 1:\n",
    "        from_ = to_ + timedelta(days=1 + (7 * (week - 2)))\n",
    "        to_ = from_ + timedelta(days=6)\n",
    "        \n",
    "    if to_.year > year:\n",
    "        to_ = datetime(year, 12, 31)\n",
    "    \n",
    "    return format_date(from_), format_date(to_)    \n",
    "\n",
    "\n",
    "def load_data(g, save_folder):\n",
    "    dfs = [pd.read_feather(file) for file in glob(str(save_folder / g))]\n",
    "    return dfs[0].append(dfs[1:], ignore_index=True)\n",
    "\n",
    "\n",
    "def load_yearly(save_folder, from_=1979, to_=2020):\n",
    "    for i in range(from_, to_ + 1):\n",
    "        yield i, load_data(f'{i}*.feather', save_folder)\n",
    "\n",
    "\n",
    "def load_monthly(save_folder, from_=197901, to_=202012):\n",
    "    while from_ <= to_:\n",
    "        yield from_, load_data(f'{from_}*.feather', save_folder)\n",
    "        from_ = increase_month(from_)\n",
    "        \n",
    "def load_weekly(save_folder, from_=197901, to_=202053):    \n",
    "    while from_ <= to_:\n",
    "        d1, d2 = week_range(from_)\n",
    "        \n",
    "        dfs = []\n",
    "        while d1 <= d2:\n",
    "            dfs.append(load_data(f'{d1}*.feather', save_folder))\n",
    "            d1 = increase_day(d1)\n",
    "            \n",
    "        yield from_, dfs[0].append(dfs[1:], ignore_index=True)\n",
    "        \n",
    "        from_ = increase_week(from_)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collapsed_group(group, group_cols, dist_cols):\n",
    "    time_value, df = group\n",
    "    \n",
    "    df[group_cols] = df[group_cols].fillna('Not Available')\n",
    "    collapsed = get_collapsed(df, group_cols, dist_cols)\n",
    "    collapsed['time_value'] = time_value\n",
    "    \n",
    "    return collapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_funcs = {\n",
    "    'yearly': {'f': load_yearly},\n",
    "    'monthly': {'f': load_monthly},\n",
    "    'weekly': {'f': load_weekly, 'batch': 42}\n",
    "}\n",
    "groups = {\n",
    "    'country': list(set(group_cols + ['adm0'])),\n",
    "    'edo': list(set(group_cols + ['adm0', 'adm1'])),\n",
    "    'mun': list(set(group_cols + ['adm0', 'adm1', 'adm2']))\n",
    "}\n",
    "all_group_cols = list(set(group_cols + [country_col, 'adm0', 'adm1', 'adm2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "monthly: 504it [1:24:44, 10.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending country\n",
      "Saving df with shape (6588341, 24)\n",
      "Appending edo\n",
      "Saving df with shape (24228382, 25)\n",
      "Appending mun\n",
      "Saving df with shape (46140368, 26)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "weekly: 2226it [1:56:28,  3.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending country\n",
      "Saving df with shape (16993811, 24)\n",
      "Appending edo\n",
      "Saving df with shape (50478266, 25)\n",
      "Appending mun\n",
      "Saving df with shape (83162071, 26)\n"
     ]
    }
   ],
   "source": [
    "processed = [Path(x).name for x in glob(str(collapse_folder / '*.feather'))]\n",
    "\n",
    "# For each time aggregation\n",
    "for time_name, time_attrs in time_funcs.items():\n",
    "    n_batches = time_attrs.get('batch', 1)\n",
    "    batch_size = 42 // n_batches + int(42 % n_batches > 0)\n",
    "    \n",
    "    collapsed = {group_name: [] for group_name in groups}\n",
    "    \n",
    "    # If all groups for a time value has been processed, no need to load the data\n",
    "    if sum([time_name in x for x in processed]) == len(groups):\n",
    "        continue\n",
    "    \n",
    "    # Load the data and collapse each group\n",
    "    for group in tqdm(time_attrs['f'](save_folder), desc=f\"{time_name}\"):\n",
    "\n",
    "        for group_name, group_cols in groups.items():  \n",
    "            fname = f'GDELT_{group_name}_{time_name}.feather'\n",
    "            if fname in processed:\n",
    "                continue\n",
    "\n",
    "            collapsed[group_name].append(\n",
    "                get_collapsed_group(group, group_cols=group_cols, dist_cols=dist_cols)\n",
    "            )\n",
    "\n",
    "    # Append and save collapsed\n",
    "    for group_name, dfs in collapsed.items():\n",
    "        fname = f'GDELT_{group_name}_{time_name}.feather'\n",
    "        print(f'Appending {group_name}')\n",
    "        df = dfs[0].append(dfs[1:], ignore_index=True)\n",
    "\n",
    "        print('Saving df with shape', df.shape)\n",
    "        df.to_feather(collapse_folder / fname)\n",
    "        del df\n",
    "\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key in groups.keys():\n",
    "#     print(f'GDELT {key} weekly')\n",
    "#     df = load_data(f'GDELT_{key}_weekly_*.feather', collapse_folder)\n",
    "#     print(df.shape)\n",
    "#     df.to_csv(collapse_folder / f'GDELT_{key}_weekly.csv')\n",
    "#     del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GDELT_edo_yearly.feather\n",
      "GDELT_country_weekly.feather\n",
      "GDELT_mun_weekly.feather\n",
      "GDELT_edo_weekly.feather\n",
      "GDELT_country_monthly.feather\n",
      "GDELT_mun_monthly.feather\n",
      "GDELT_country_yearly.feather\n",
      "GDELT_edo_monthly.feather\n",
      "GDELT_mun_yearly.feather\n"
     ]
    }
   ],
   "source": [
    "# Transform feather to csv\n",
    "all_files = [Path(f) for f in glob(str(collapse_folder / '*.feather'))]\n",
    "feather_files = [x for x in all_files if x.suffix == '.feather']\n",
    "names = {x.name for x in feather_files}\n",
    "for file in feather_files:\n",
    "    csv_file = file.with_suffix('.csv')\n",
    "    if csv_file.name in names or '_b' in file.name:\n",
    "        continue\n",
    "    print(file.name)\n",
    "    df = pd.read_feather(file)\n",
    "    df.to_csv(csv_file, index=False)\n",
    "    del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luisdasilva/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3071: DtypeWarning: Columns (1) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "GG, RB, US, YI\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(collapse_folder / f'GDELT_mun_weekly.csv')\n",
    "\n",
    "no_adm = df.loc[df['adm1'] == 'Not Available', 'adm0'].unique()\n",
    "with_adm = df.loc[df['adm1'] != 'Not Available', 'adm0'].unique()\n",
    "not_available = [x for x in set(no_adm).difference(with_adm) if len(x) < 3]\n",
    "print(len(not_available))\n",
    "print(', '.join(sorted(not_available)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../Output/GDELT_collapsed/GDELT_edo_monthly.csv',\n",
       " '../Output/GDELT_collapsed/GDELT_country_yearly.csv',\n",
       " '../Output/GDELT_collapsed/GDELT_mun_monthly.csv',\n",
       " '../Output/GDELT_collapsed/GDELT_mun_yearly.csv',\n",
       " '../Output/GDELT_collapsed/GDELT_mun_weekly.csv',\n",
       " '../Output/GDELT_collapsed/GDELT_country_weekly.csv',\n",
       " '../Output/GDELT_collapsed/GDELT_edo_weekly.csv',\n",
       " '../Output/GDELT_collapsed/GDELT_country_monthly.csv',\n",
       " '../Output/GDELT_collapsed/GDELT_edo_yearly.csv']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob(str(collapse_folder / '*.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 4/9 [19:23<25:22, 304.54s/it]"
     ]
    }
   ],
   "source": [
    "country_map = {'GG': 'DEU', 'US': 'USA'}\n",
    "for file in tqdm(glob(str(collapse_folder / '*.csv'))):\n",
    "    df = pd.read_csv(file, dtype=str)\n",
    "    \n",
    "    try:\n",
    "        df.drop(columns='Unnamed: 0', inplace=True)\n",
    "    except KeyError:\n",
    "        pass\n",
    "    \n",
    "    df['adm0'] = df['adm0'].apply(lambda x: country_map.get(x, x))\n",
    "    df.to_csv(file, index=False)\n",
    "    del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 505/505 [59:24<00:00,  7.06s/it]  \n"
     ]
    }
   ],
   "source": [
    "files_by_month = {}\n",
    "\n",
    "for file in iglob(str(save_folder / '*.feather')):\n",
    "    file = Path(file)\n",
    "    date = file.name[:6]\n",
    "    if not date.isnumeric():\n",
    "        print('Skipping', file.name)\n",
    "        continue\n",
    "        \n",
    "    if date in files_by_month:\n",
    "        files_by_month[date].append(file)\n",
    "    else:\n",
    "        files_by_month[date] = [file]\n",
    "        \n",
    "for date, files in tqdm(files_by_month.items()):\n",
    "    with ZipFile(save_folder / f'{date}.zip', 'w', compression=ZIP_DEFLATED) as zf:\n",
    "        for file in files:\n",
    "            zf.write(file, file.name)\n",
    "            \n",
    "    for file in files:\n",
    "        os.remove(file)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob(str(save_folder) + '/*.zip')\n",
    "\n",
    "dfs = []\n",
    "np.random.seed(43)\n",
    "for zfile in np.random.choice(files, 20):\n",
    "    with ZipFile(str(zfile), 'r') as zf:\n",
    "        dfiles = zf.namelist()\n",
    "        if len(dfiles) > 5:\n",
    "            dfiles = [str(x) for x in np.random.choice(dfiles, 1)]\n",
    "        for dfile in dfiles:\n",
    "            zf.extract(str(dfile), '')\n",
    "            dfile = Path(dfile)\n",
    "            dfs.append(pd.read_feather(str(dfile)))\n",
    "            os.remove(str(dfile))\n",
    "            \n",
    "df = dfs[0].append(dfs[1:])\n",
    "del dfs\n",
    "\n",
    "df['check_passed'] = np.nan\n",
    "df[df['adm2'].notnull()].sample(100).to_excel(save_folder / 'sanity_check_gdelt.xlsx', index=False)\n",
    "# subdf.reset_index(drop=True).to_feather(save_folder / f'{date}_{n:02}.feather')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
