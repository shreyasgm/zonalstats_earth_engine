{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import tarfile\n",
    "import re\n",
    "\n",
    "from functools import partial\n",
    "# from multiprocessing import Pool\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from glob import glob, iglob\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from unittest.mock import MagicMock\n",
    "\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from projections.shapefiles import iter_records\n",
    "from projections import raster, utils\n",
    "\n",
    "\n",
    "pd.set_option('max_columns', None)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def load_tifs(tars):\n",
    "    for tar in tars:\n",
    "        names = []\n",
    "\n",
    "        with tarfile.open(tar, 'r') as tf:\n",
    "            # Find gz in tar\n",
    "            names = [x for x in tf.getnames() if x.endswith('.gz') and 'stable_lights' in x]\n",
    "            if not names:\n",
    "                print(f'No stable lights in {tar}')\n",
    "                continue\n",
    "\n",
    "            for name in names:\n",
    "                tf.extract(name, read_path)\n",
    "\n",
    "        for name in names:\n",
    "            # Extract tif\n",
    "            name_path = read_path / name\n",
    "            os.system(f'gunzip {name_path}')\n",
    "\n",
    "            tifs = glob(str(read_path / '*.tif'))\n",
    "\n",
    "            for tif in tifs:\n",
    "                # Yield path to tif file\n",
    "                yield tif\n",
    "\n",
    "            # Clean up\n",
    "            for file in tifs + [name_path]:\n",
    "                try:\n",
    "                    os.remove(file)\n",
    "                except FileNotFoundError:\n",
    "                    continue\n",
    "                    \n",
    "                    \n",
    "def save_location_mapping(row_and_path):\n",
    "    row, path = row_and_path\n",
    "    shape = row['geometry']\n",
    "    \n",
    "    subset = raster.find_subset_with_intersection_area(IMAGE, shape)\n",
    "\n",
    "    if subset is None:\n",
    "        with open(path, 'w') as f:\n",
    "            f.write('')\n",
    "        return\n",
    "\n",
    "    subset['adm0'] = row['GID_0']\n",
    "    subset['adm1'] = row['GID_1']\n",
    "    subset['adm2'] = row['GID_2']\n",
    "    \n",
    "    subset.to_csv(path, index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "read_path = Path('../Data/Nightlights/')\n",
    "output_path = Path('../Output/Nighlights/')\n",
    "partial_path = output_path / 'partial_locs'\n",
    "countries_path = output_path / 'countries'\n",
    "ethnic_path = output_path / 'ethnic'\n",
    "ethnic_countries_path = output_path / 'ethnic_countries'\n",
    "\n",
    "tars = sorted(glob(str(read_path / '*.tar')))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "geo_df = gpd.read_file('../Shapefiles/preprocessed/all_countries.shp')\n",
    "geo_df['GID_1'].fillna(geo_df['GID_0'], inplace=True)\n",
    "geo_df['GID_2'].fillna(geo_df['GID_1'], inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Map raster to polygons"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def get_filename_from_row(row, path):\n",
    "    name = f'{row[\"GID_2\"]}.csv'\n",
    "    return path / name\n",
    "\n",
    "\n",
    "def get_filename_with_portion_from_row(row, path):\n",
    "    portion = f\"_p{int(row['portion']):03d}\" if row['portion'] else ''\n",
    "    name = f'{row[\"GID_2\"]}{portion}.csv'\n",
    "    return path / name\n",
    "\n",
    "\n",
    "def record_exists(row, path):\n",
    "    return (\n",
    "        get_filename_from_row(row, path).exists() or \n",
    "        get_filename_with_portion_from_row(row, path).exists()\n",
    "    )\n",
    "\n",
    "\n",
    "def yield_missing_records(records, save_path):\n",
    "    for _, row in records.iterrows():\n",
    "        if record_exists(row, save_path):\n",
    "            continue\n",
    "            \n",
    "        yield row, get_filename_with_portion_from_row(row, save_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Only need to map the locations once\n",
    "# Still loop for the clean up\n",
    "n_processes = 30\n",
    "for tif in load_tifs(tars[:1]):\n",
    "    IMAGE = utils.read_tif(tif)\n",
    "    processing_function = partial(save_location_mapping, crs=geo_df.crs)\n",
    "    country_iterator = partial(yield_missing_records, save_path=partial_path)\n",
    "    \n",
    "    if n_processes == 1:\n",
    "        for blob in tqdm(country_iterator(geo_df)):\n",
    "            processing_function(blob)\n",
    "    else:\n",
    "        with ThreadPoolExecutor(n_processes) as tpe:\n",
    "            for _ in tqdm(tpe.map(processing_function, country_iterator(geo_df)), \n",
    "                          total=geo_df.shape[0]):\n",
    "                pass"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "gid_with_portions = geo_df.loc[geo_df['portion'].notnull(), 'GID_2'].unique()\n",
    "for gid in tqdm(gid_with_portions):\n",
    "    files = list(partial_path.glob(f'{gid}_p*.csv'))\n",
    "    if not files:\n",
    "        continue\n",
    "        \n",
    "    portions = [utils.read_csv(file) for file in files]\n",
    "    portions = [x for x in portions if not x.empty]\n",
    "    \n",
    "    if not portions:\n",
    "        continue\n",
    "    elif len(portions) == 1:\n",
    "        country = portions[0]\n",
    "    else:\n",
    "        county = portions[0].append(portions[1:], ignore_index=True)\n",
    "        \n",
    "    county = county.groupby(['lat', 'lon']).sum().reset_index()\n",
    "    for i in range(3):\n",
    "        county[f'adm{i}'] = portions[0].iloc[0][f'adm{i}']\n",
    "    county.to_csv(partial_path / f'{gid}.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Aggregate"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def is_portion(name):\n",
    "    match = re.match('.*_p\\d\\d\\d.csv$', name)\n",
    "    return match is not None\n",
    "\n",
    "\n",
    "def get_files_by_country(path):\n",
    "    files_by_country = {}\n",
    "    for file in path.glob('*.csv'):\n",
    "        if is_portion(file.name):\n",
    "            continue\n",
    "            \n",
    "        file = Path(file)\n",
    "        country = file.name[:3]\n",
    "        if country in files_by_country:\n",
    "            files_by_country[country].append(file)\n",
    "        else:\n",
    "            files_by_country[country] = [file]\n",
    "            \n",
    "    return files_by_country\n",
    "\n",
    "\n",
    "def get_year_from_tif(tif):\n",
    "    return int(Path(tif).name[3:7])\n",
    "\n",
    "\n",
    "def aggregate_and_save(results, tif, save_path, name, groupby):\n",
    "    year = Path(tif).name[3:7]\n",
    "    \n",
    "    if '{year}' in name:\n",
    "        name = name.format(year=year)\n",
    "        \n",
    "    save_path = save_path / name\n",
    "\n",
    "    if save_path.exists():\n",
    "        return False\n",
    "\n",
    "    # Get raster subset\n",
    "    mask = results['lon'] > -999\n",
    "    \n",
    "    if mask.sum() > 0:\n",
    "        increment = raster.get_increment_from_tif(IMAGE)\n",
    "        \n",
    "        shape = MagicMock()\n",
    "        shape.bounds = (\n",
    "            results.loc[mask, 'lon'].min() - increment,\n",
    "            results.loc[mask, 'lat'].min() - increment,\n",
    "            results['lon'].max() + increment,\n",
    "            results['lat'].max() + increment\n",
    "        )\n",
    "\n",
    "        subset = raster.get_df_by_maximum_bounds(IMAGE, shape, geo=False)\n",
    "        subset['lon'] = np.round(subset['lon'], 6)\n",
    "        subset['lat'] = np.round(subset['lat'], 6)\n",
    "\n",
    "        # Combine with results\n",
    "        tresults = results.merge(subset, on=['lon', 'lat'], how='left')\n",
    "        tresults['value'].fillna(0, inplace=True)  # This fills the -999\n",
    "    else:\n",
    "        tresults = results.copy()\n",
    "        tresults['value'] = 0\n",
    "\n",
    "    # Aggregate\n",
    "    pivot = tresults.groupby(groupby)[['intersection_area', 'value']].sum()\n",
    "    pivot['year'] = int(year)\n",
    "\n",
    "    # Save\n",
    "    pivot.reset_index().to_csv(save_path, index=False)\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "def load_nl_files(files):\n",
    "    results = []\n",
    "    for file in files:\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "        except pd.errors.EmptyDataError:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            results.append(df[pd.to_numeric(df['intersection_area']) > 0])    \n",
    "        except Exception as e:\n",
    "            print(file)\n",
    "            raise e\n",
    "        \n",
    "    results = results[0].append(results[1:], ignore_index=True)\n",
    "    results['lon'] = np.round(results['lon'], 6)\n",
    "    results['lat'] = np.round(results['lat'], 6)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def iter_files_by_country(files_by_country, tif, path):\n",
    "    for country, files in files_by_country.items():\n",
    "        year = get_year_from_tif(tif)\n",
    "        name = f'{country}_{year}.csv'\n",
    "        if (path / name).exists():\n",
    "            continue\n",
    "            \n",
    "        yield country, files, name, tif"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def load_aggregate_and_save(blob):\n",
    "    country, files, name, tif_name = blob    \n",
    "    \n",
    "    # Load polygons\n",
    "    results = load_nl_files(files)\n",
    "    results['adm1'].fillna('', inplace=True)\n",
    "    results['adm2'].fillna('', inplace=True)\n",
    "    results.drop_duplicates(inplace=True)\n",
    "\n",
    "    aggregate_and_save(results, tif, countries_path, name, groupby=['adm0', 'adm1', 'adm2'])\n",
    "\n",
    "    \n",
    "n_processes = 10\n",
    "files_by_country = get_files_by_country(partial_path)\n",
    "\n",
    "with ThreadPoolExecutor(n_processes) as tpe:\n",
    "    for tif in load_tifs(tars):\n",
    "        IMAGE = utils.read_tif(tif)\n",
    "        for _ in tqdm(tpe.map(load_aggregate_and_save, \n",
    "                              iter_files_by_country(files_by_country, tif, countries_path)), \n",
    "                      total=len(files_by_country), desc=tif):\n",
    "            pass"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "../Data/Nightlights/F152000.v4b_web.stable_lights.avg_vis.tif: 100%|██████████| 256/256 [26:19<00:00,  6.17s/it] \n",
      "../Data/Nightlights/F101992.v4b_web.stable_lights.avg_vis.tif: 100%|██████████| 256/256 [25:55<00:00,  6.08s/it] \n",
      "../Data/Nightlights/F101993.v4b_web.stable_lights.avg_vis.tif: 100%|██████████| 256/256 [24:43<00:00,  5.79s/it] \n",
      "../Data/Nightlights/F121994.v4b_web.stable_lights.avg_vis.tif: 100%|██████████| 256/256 [24:57<00:00,  5.85s/it] \n",
      "../Data/Nightlights/F121995.v4b_web.stable_lights.avg_vis.tif: 100%|██████████| 256/256 [25:28<00:00,  5.97s/it] \n",
      "../Data/Nightlights/F121996.v4b_web.stable_lights.avg_vis.tif: 100%|██████████| 256/256 [25:24<00:00,  5.96s/it] \n",
      "../Data/Nightlights/F141997.v4b_web.stable_lights.avg_vis.tif: 100%|██████████| 256/256 [25:04<00:00,  5.88s/it] \n",
      "../Data/Nightlights/F141998.v4b_web.stable_lights.avg_vis.tif: 100%|██████████| 256/256 [24:36<00:00,  5.77s/it] \n",
      "../Data/Nightlights/F141999.v4b_web.stable_lights.avg_vis.tif: 100%|██████████| 256/256 [25:25<00:00,  5.96s/it] \n",
      "../Data/Nightlights/F152000.v4b_web.stable_lights.avg_vis.tif:   0%|          | 0/256 [00:00<?, ?it/s]\n",
      "../Data/Nightlights/F152001.v4b_web.stable_lights.avg_vis.tif: 100%|██████████| 256/256 [25:01<00:00,  5.87s/it] \n",
      "../Data/Nightlights/F152002.v4b_web.stable_lights.avg_vis.tif: 100%|██████████| 256/256 [24:40<00:00,  5.78s/it] \n",
      "../Data/Nightlights/F152003.v4b_web.stable_lights.avg_vis.tif: 100%|██████████| 256/256 [25:27<00:00,  5.97s/it] \n",
      "../Data/Nightlights/F162004.v4b_web.stable_lights.avg_vis.tif: 100%|██████████| 256/256 [25:22<00:00,  5.95s/it] \n",
      "../Data/Nightlights/F162005.v4b_web.stable_lights.avg_vis.tif: 100%|██████████| 256/256 [24:55<00:00,  5.84s/it] \n",
      "../Data/Nightlights/F162006.v4b_web.stable_lights.avg_vis.tif: 100%|██████████| 256/256 [25:14<00:00,  5.92s/it] \n",
      "../Data/Nightlights/F162007.v4b_web.stable_lights.avg_vis.tif: 100%|██████████| 256/256 [25:05<00:00,  5.88s/it] \n",
      "../Data/Nightlights/F162008.v4b_web.stable_lights.avg_vis.tif: 100%|██████████| 256/256 [25:02<00:00,  5.87s/it] \n",
      "../Data/Nightlights/F162009.v4b_web.stable_lights.avg_vis.tif: 100%|██████████| 256/256 [25:05<00:00,  5.88s/it] \n",
      "../Data/Nightlights/F182010.v4d_web.stable_lights.avg_vis.tif: 100%|██████████| 256/256 [24:54<00:00,  5.84s/it] \n",
      "../Data/Nightlights/F182011.v4c_web.stable_lights.avg_vis.tif: 100%|██████████| 256/256 [25:04<00:00,  5.88s/it] \n",
      "../Data/Nightlights/F182012.v4c_web.stable_lights.avg_vis.tif: 100%|██████████| 256/256 [25:06<00:00,  5.88s/it] \n",
      "../Data/Nightlights/F182013.v4c_web.stable_lights.avg_vis.tif: 100%|██████████| 256/256 [24:41<00:00,  5.79s/it] \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Combine aggregations"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "results = []\n",
    "for file in tqdm(countries_path.glob('*.csv')):\n",
    "    df = utils.read_csv(file)\n",
    "    if not df.empty:\n",
    "        results.append(df)\n",
    "        \n",
    "results = results[0].append(results[1:], ignore_index=True)\n",
    "print(results.shape)\n",
    "results.to_csv(output_path / 'nightlights.csv', index=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "5632it [00:16, 349.05it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1027950, 6)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ethnic\n",
    "# Map raster to Ethnic locs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "def iter_records(adm, save_path):\n",
    "    for _, row in tqdm(adm.iterrows(), total=adm.shape[0]):\n",
    "        name = f'{row[\"GID_0\"]}_{row[\"NAME\"]}.csv'\n",
    "        name_path = Path(save_path) / name\n",
    "            \n",
    "        yield row, name_path\n",
    "\n",
    "def iter_and_skip_records(adm, save_path):\n",
    "    for row, name_path in iter_records(adm, save_path):\n",
    "        if name_path.exists():\n",
    "            continue\n",
    "            \n",
    "        yield row, name_path    \n",
    "        \n",
    "\n",
    "def save_location_mapping(blob):\n",
    "    row, name_path = blob\n",
    "    shape = row['geometry']\n",
    "    \n",
    "    subset = raster.find_subset_with_intersection_area(IMAGE, shape)\n",
    "    if subset is None:\n",
    "        print('No intersection found for', name_path)\n",
    "        with open(name_path, 'w') as f:\n",
    "            f.write('')\n",
    "        return\n",
    "\n",
    "    for col in ['NAME', 'TRIBE_CODE', 'GID_0']:\n",
    "        subset[col] = row[col]\n",
    "    \n",
    "    subset.to_csv(name_path, index=False)\n",
    "    \n",
    "# Load shapes\n",
    "adm = gpd.read_file('../Shapefiles/ethnic_preprocessed/tribe_adm0_s.shp')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Map locations\n",
    "n_processes = 15\n",
    "for tif in load_tifs(tars[:1]):\n",
    "    IMAGE = utils.read_tif(tif)\n",
    "    i = partial(iter_and_skip_records, save_path=ethnic_path)\n",
    "    \n",
    "    if n_processes == 1:\n",
    "        for blob in i(adm):\n",
    "            save_location_mapping(blob)\n",
    "    else:\n",
    "        with ThreadPoolExecutor(n_processes) as tpe:\n",
    "            for _ in tqdm(tpe.map(save_location_mapping, i(adm)), total=adm.shape[0]):\n",
    "                pass"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 5053/5053 [00:01<00:00, 3224.31it/s]\n",
      "  0%|          | 0/5053 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "gid_with_portions = adm.loc[adm['portion'].notnull(), ['GID_0', 'NAME']].agg('_'.join, axis=1).unique()\n",
    "for gid in tqdm(gid_with_portions):\n",
    "    files = list(ethnic_path.glob(f'{gid}_p*.csv'))\n",
    "    if not files:\n",
    "        continue\n",
    "        \n",
    "    portions = [utils.read_csv(file) for file in files]\n",
    "    portions = [x for x in portions if not x.empty]\n",
    "    \n",
    "    if not portions:\n",
    "        continue\n",
    "    elif len(portions) == 1:\n",
    "        country = portions[0]\n",
    "    else:\n",
    "        country = portions[0].append(portions[1:], ignore_index=True)\n",
    "        \n",
    "    country = country.groupby(['lat', 'lon']).sum().reset_index()\n",
    "    l\n",
    "    for i in range(3):\n",
    "        country[f'adm{i}'] = portions[0].iloc[0][f'adm{i}']\n",
    "    country.to_csv(partial_path / f'{gid}.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Aggregate ethnic"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "def ethnic_load_aggregate_and_save(blob):\n",
    "    groupby = ['NAME', 'TRIBE_CODE', 'GID_0']\n",
    "    country, files, name, tif_name = blob    \n",
    "    \n",
    "    # Load polygons\n",
    "    results = load_nl_files(files)\n",
    "    for col in groupby:\n",
    "        results[col].fillna('', inplace=True)\n",
    "    results.drop_duplicates(inplace=True)\n",
    "\n",
    "    aggregate_and_save(results, tif_name, ethnic_countries_path, name, groupby=groupby)\n",
    "\n",
    "\n",
    "files_by_country = get_files_by_country(ethnic_path)\n",
    "\n",
    "n_processes = 10\n",
    "with ThreadPoolExecutor(n_processes) as tpe:\n",
    "    for tif in load_tifs(tars):\n",
    "        IMAGE = utils.read_tif(tif)\n",
    "        for _ in tqdm(tpe.map(ethnic_load_aggregate_and_save, \n",
    "                              iter_files_by_country(files_by_country, tif, ethnic_countries_path)), \n",
    "                      total=len(files_by_country), desc=tif):\n",
    "            pass"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "../Data/Nightlights/F101992.v4b_web.stable_lights.avg_vis.tif: 100%|██████████| 57/57 [01:44<00:00,  1.83s/it]\n",
      "../Data/Nightlights/F101993.v4b_web.stable_lights.avg_vis.tif: 100%|██████████| 57/57 [01:44<00:00,  1.84s/it]\n",
      "../Data/Nightlights/F121994.v4b_web.stable_lights.avg_vis.tif: 100%|██████████| 57/57 [01:44<00:00,  1.84s/it]\n",
      "../Data/Nightlights/F121995.v4b_web.stable_lights.avg_vis.tif: 100%|██████████| 57/57 [01:44<00:00,  1.83s/it]\n",
      "../Data/Nightlights/F121996.v4b_web.stable_lights.avg_vis.tif: 100%|██████████| 57/57 [01:44<00:00,  1.83s/it]\n",
      "../Data/Nightlights/F141997.v4b_web.stable_lights.avg_vis.tif: 100%|██████████| 57/57 [01:44<00:00,  1.83s/it]\n",
      "../Data/Nightlights/F141998.v4b_web.stable_lights.avg_vis.tif: 100%|██████████| 57/57 [01:44<00:00,  1.83s/it]\n",
      "../Data/Nightlights/F141999.v4b_web.stable_lights.avg_vis.tif: 100%|██████████| 57/57 [01:44<00:00,  1.83s/it]\n",
      "../Data/Nightlights/F152000.v4b_web.stable_lights.avg_vis.tif: 100%|██████████| 57/57 [01:43<00:00,  1.82s/it]\n",
      "../Data/Nightlights/F152001.v4b_web.stable_lights.avg_vis.tif: 100%|██████████| 57/57 [01:45<00:00,  1.85s/it]\n",
      "../Data/Nightlights/F152002.v4b_web.stable_lights.avg_vis.tif: 100%|██████████| 57/57 [01:44<00:00,  1.83s/it]\n",
      "../Data/Nightlights/F152003.v4b_web.stable_lights.avg_vis.tif: 100%|██████████| 57/57 [01:45<00:00,  1.84s/it]\n",
      "../Data/Nightlights/F162004.v4b_web.stable_lights.avg_vis.tif: 100%|██████████| 57/57 [01:44<00:00,  1.84s/it]\n",
      "../Data/Nightlights/F162005.v4b_web.stable_lights.avg_vis.tif: 100%|██████████| 57/57 [01:45<00:00,  1.86s/it]\n",
      "../Data/Nightlights/F162006.v4b_web.stable_lights.avg_vis.tif: 100%|██████████| 57/57 [01:44<00:00,  1.83s/it]\n",
      "../Data/Nightlights/F162007.v4b_web.stable_lights.avg_vis.tif: 100%|██████████| 57/57 [01:44<00:00,  1.83s/it]\n",
      "../Data/Nightlights/F162008.v4b_web.stable_lights.avg_vis.tif: 100%|██████████| 57/57 [01:45<00:00,  1.85s/it]\n",
      "../Data/Nightlights/F162009.v4b_web.stable_lights.avg_vis.tif: 100%|██████████| 57/57 [01:44<00:00,  1.84s/it]\n",
      "../Data/Nightlights/F182010.v4d_web.stable_lights.avg_vis.tif: 100%|██████████| 57/57 [01:44<00:00,  1.84s/it]\n",
      "../Data/Nightlights/F182011.v4c_web.stable_lights.avg_vis.tif: 100%|██████████| 57/57 [01:44<00:00,  1.82s/it]\n",
      "../Data/Nightlights/F182012.v4c_web.stable_lights.avg_vis.tif: 100%|██████████| 57/57 [01:44<00:00,  1.83s/it]\n",
      "../Data/Nightlights/F182013.v4c_web.stable_lights.avg_vis.tif: 100%|██████████| 57/57 [01:44<00:00,  1.84s/it]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "results = []\n",
    "for file in tqdm(ethnic_countries_path.glob('*.csv')):\n",
    "    try:\n",
    "        results.append(pd.read_csv(file))\n",
    "    except pd.errors.EmptyDataError:\n",
    "        continue\n",
    "results = results[0].append(results[1:], ignore_index=True)\n",
    "print(results.shape)\n",
    "results.drop_duplicates(['NAME', 'GID_0', 'year']).to_csv(output_path / 'nightlights_ethnic.csv', index=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "1254it [00:02, 469.53it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(31064, 6)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}