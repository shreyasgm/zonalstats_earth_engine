{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sd2/Moncho/zonalstats_earth_engine/.venv/lib/python3.8/site-packages/geopandas/_compat.py:106: UserWarning: The Shapely GEOS version (3.8.0-CAPI-1.13.1 ) is incompatible with the GEOS version PyGEOS was compiled with (3.9.1-CAPI-1.14.2). Conversions between both will be slow.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "import yaml\n",
    "\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from projections.elevation import get_indices_by_file\n",
    "from projections import raster, utils\n",
    "\n",
    "\n",
    "pd.set_option('max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_location_mapping(row_path):\n",
    "    \"\"\"\n",
    "    Common function used to obtain a mapping of polygons to\n",
    "    the rasters used by the IMAGE. \n",
    "    \n",
    "    This is not part of projections because it assumes IMAGE \n",
    "    exists in the global scope.\n",
    "    \"\"\"\n",
    "    row, path = row_path\n",
    "    shape = row['geometry']\n",
    "    \n",
    "    subset = raster.find_subset_with_intersection_area(IMAGE, shape)\n",
    "\n",
    "    if subset.empty:\n",
    "        with open(path, 'w') as f:\n",
    "            f.write('')\n",
    "        return\n",
    "\n",
    "    subset['id'] = row['id']\n",
    "    \n",
    "    subset.to_csv(path, index=False)\n",
    "    \n",
    "    \n",
    "def yield_missing_shapes(gdf, save_path, prefix):\n",
    "    for _, row in gdf.iterrows():\n",
    "        path = save_path / get_save_file_name(prefix, row)\n",
    "        if path.exists():\n",
    "            continue\n",
    "            \n",
    "        yield row, path\n",
    "        \n",
    "\n",
    "def get_save_file_name(prefix, row):\n",
    "    portion = f\"_p{row['portion']}\" if row['portion'] else ''\n",
    "    return f'{prefix}_{row[\"id\"]}{portion}.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_path = Path('../Data/Elevation/GTOPO30')\n",
    "output_path = Path('../Output/Elevation/GTOPO30/')\n",
    "partial_path = output_path / 'partial'\n",
    "by_country_path = output_path / 'by_country'\n",
    "\n",
    "output_path.mkdir(exist_ok=True)\n",
    "partial_path.mkdir(exist_ok=True)\n",
    "by_country_path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map raster to polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read SHP with all polygons (output of PreprocessingLocs.ipynb, also available in Drive)\n",
    "geo_df = gpd.read_file('../Shapefiles/preprocessed/all_countries_with_eth.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_file = read_path / 'indices_by_file.yml'\n",
    "if indices_file.exists():\n",
    "    with open(indices_file, 'r') as f:\n",
    "        indices_by_file = yaml.safe_load(f)\n",
    "else:\n",
    "    files = list(read_path.glob('gt30e*.tif')) + list(read_path.glob('gt30w*.tif'))\n",
    "    indices_by_file = get_indices_by_file(geo_df[geo_df['id'] != 'ATA'], files)\n",
    "\n",
    "    with open(read_path / 'indices_by_file.yml', 'w') as f:\n",
    "        yaml.dump(indices_by_file, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|█▋                              | 6625/122772 [1:45:29<30:49:34,  1.05it/s]\n",
      "  7%|██▍                             | 9125/122772 [1:02:40<13:00:40,  2.43it/s]\n",
      "  2%|▌                               | 2124/122772 [1:18:52<74:40:04,  2.23s/it]\n",
      "  2%|▋                               | 2859/122772 [1:33:58<65:41:41,  1.97s/it]\n",
      "  4%|█▎                                | 4540/122772 [39:11<17:00:50,  1.93it/s]\n",
      "  0%|                                  | 6/122772 [18:14<6222:22:14, 182.47s/it]\n",
      "  0%|                                                | 0/122772 [00:00<?, ?it/s]\n",
      "  5%|█▊                                | 6343/122772 [56:53<17:24:22,  1.86it/s]\n",
      "  3%|█                                 | 3973/122772 [35:09<17:31:14,  1.88it/s]\n",
      "  2%|▊                                 | 2845/122772 [19:49<13:55:32,  2.39it/s]\n",
      "  0%|                                                | 0/122772 [00:00<?, ?it/s]\n",
      "  2%|▌                                 | 1899/122772 [17:54<19:00:07,  1.77it/s]\n",
      "  2%|▌                                 | 2209/122772 [14:51<13:31:06,  2.48it/s]\n",
      "  1%|▎                              | 1336/122772 [3:00:28<273:23:34,  8.10s/it]\n",
      "  0%|                                                | 0/122772 [00:00<?, ?it/s]\n",
      "  5%|█▋                              | 6613/122772 [1:07:09<19:39:45,  1.64it/s]\n",
      "  4%|█▍                              | 5311/122772 [1:53:56<42:00:09,  1.29s/it]\n",
      "  0%|▏                                 | 461/122772 [28:38<126:37:01,  3.73s/it]\n",
      "  2%|▌                                 | 1955/122772 [17:34<18:06:08,  1.85it/s]\n",
      "  7%|██▎                                | 8282/122772 [31:54<7:21:02,  4.33it/s]\n",
      "  4%|█▎                                 | 4552/122772 [20:17<8:47:10,  3.74it/s]\n",
      "  0%|                                                | 0/122772 [00:00<?, ?it/s]\n",
      "  6%|█▉                                | 6904/122772 [52:57<14:48:43,  2.17it/s]\n",
      " 16%|█████                           | 19199/122772 [1:42:53<9:15:03,  3.11it/s]\n",
      "  5%|█▍                             | 5868/122772 [5:26:59<108:34:25,  3.34s/it]\n",
      "  0%|                                                | 0/122772 [00:00<?, ?it/s]\n",
      "  1%|▎                                 | 1285/122772 [45:22<71:30:27,  2.12s/it]\n",
      "  6%|█▉                              | 7211/122772 [1:58:31<31:39:34,  1.01it/s]\n",
      "  0%|                                    | 84/122772 [00:36<14:41:56,  2.32it/s]\n",
      "  0%|                                    | 81/122772 [02:22<59:48:22,  1.75s/it]\n",
      "  3%|▉                                  | 3322/122772 [12:48<7:40:31,  4.32it/s]\n",
      "  1%|▎                                   | 932/122772 [03:12<6:58:38,  4.85it/s]\n",
      "  0%|                                                | 0/122772 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "n_processes = 30\n",
    "for file, indices in indices_by_file.items():\n",
    "    IMAGE = utils.read_tif(read_path / file)\n",
    "    iterator = partial(\n",
    "        yield_missing_shapes, \n",
    "        save_path=partial_path, \n",
    "        prefix=file[:-4],\n",
    "    )\n",
    "    \n",
    "    if n_processes == 1:\n",
    "        for row_and_path in tqdm(iterator(geo_df.loc[indices])):\n",
    "            save_location_mapping(row_and_path)\n",
    "    else:\n",
    "        with ThreadPoolExecutor(n_processes) as tpe:\n",
    "            for _ in tqdm(\n",
    "                tpe.map(save_location_mapping, iterator(geo_df.loc[indices])), \n",
    "                total=geo_df.shape[0]\n",
    "            ):\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading: 116303it [08:50, 219.08it/s]\n",
      "Saving: 100%|████████████████████████████| 47649/47649 [02:44<00:00, 289.49it/s]\n"
     ]
    }
   ],
   "source": [
    "utils.union_and_save_portions(read_from=partial_path, save_in=by_country_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate\n",
    "Read the preprocessed results, aggregate them and save a consolidated file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grouping: 47649it [06:21, 125.05it/s]\n"
     ]
    }
   ],
   "source": [
    "countries = {}\n",
    "for file in tqdm(by_country_path.glob('*.feather'), desc='Grouping'):\n",
    "    df = pd.read_feather(file)\n",
    "    df = df[df['value'] > -9999].copy()\n",
    "    if df.empty:\n",
    "        continue\n",
    "    df = utils.get_weighted_average(df, value='value', weight='intersection_area', by=['id'])\n",
    "    country = df.loc[0, 'id']\n",
    "    countries.setdefault(country, []).append(df)\n",
    "\n",
    "dfs = [utils.combine_dataframes(country_dfs)\n",
    "      for country_dfs in countries.values()]\n",
    "df = utils.combine_dataframes(dfs)\n",
    "df.rename(columns={'_weighted_value_': 'value'}, inplace=True)\n",
    "df.to_csv(output_path / 'elevation.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
