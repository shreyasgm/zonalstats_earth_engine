{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "from functools import partial\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "\n",
    "import shapefile\n",
    "import shapely\n",
    "from shapely.geometry import Point\n",
    "\n",
    "pd.set_option('max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lon = 'lon'\n",
    "lat = 'lat'\n",
    "country = 'country'\n",
    "output_folder = Path('../Output/Precipitaciones/GPCC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "\n",
    "for file in tqdm(glob('../Data/Precipitaciones/full_data_monthly_v2020_*.csv')):\n",
    "    df = pd.read_csv(file)\n",
    "    dfs.append(df[['lon', 'lat']].copy())\n",
    "    del df\n",
    "    \n",
    "df = dfs[0].append(dfs[1:])\n",
    "del dfs\n",
    "\n",
    "print(df.shape)\n",
    "print('Unique locs', df.drop_duplicates(['lon', 'lat']).shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found, loading locs from df\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    locs = pd.read_csv(output_folder / 'unique_locs_gpcc.csv')\n",
    "except FileNotFoundError:\n",
    "    print('File not found, loading locs from df')\n",
    "    locs = df[[lon, lat]].drop_duplicates()\n",
    "    locs.loc[:, 'adm0'] = np.nan\n",
    "    locs.loc[:, 'adm1'] = np.nan\n",
    "    locs.loc[:, 'adm2'] = np.nan\n",
    "    locs.loc[:, 'nearest_loc'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 17/276 [00:17<10:52,  2.52s/it]"
     ]
    }
   ],
   "source": [
    "shps = glob('../Shapefiles/preprocessed/*.shp')\n",
    "shapes_by_country = {}\n",
    "for i, shp in tqdm(enumerate(shps), total=len(shps)):\n",
    "    codes = Path(shp).name.replace('.shp', '').split('_')\n",
    "    shp = shapefile.Reader(shp)\n",
    "    shapes = [shapely.geometry.shape(s) for s in shp.shapes()]\n",
    "    records = shp.records()\n",
    "    \n",
    "    for code in codes:\n",
    "        shapes_by_country[code] = []\n",
    "        for shape, record in zip(shapes, records):\n",
    "            shapes_by_country[code].append((shape, *record))\n",
    "    \n",
    "assert 'GT' in shapes_by_country and 'PM' in shapes_by_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 284913/284913 [6:07:30<00:00, 12.92it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total nearest: 45031\n"
     ]
    }
   ],
   "source": [
    "def country_shapes(country=None):\n",
    "    if country is not None and country in shapes_by_country:\n",
    "        print('Found country', country)\n",
    "        yield from shapes_by_country[country]\n",
    "    else:\n",
    "        for shapes in shapes_by_country.values():\n",
    "            yield from shapes\n",
    "            \n",
    "            \n",
    "def match_single_geo(row, lon_col, lat_col, country_col):\n",
    "    if isinstance(row, tuple):\n",
    "        row = row[1]\n",
    "        \n",
    "    if isinstance(row['adm1'], str):\n",
    "        return {'nearest_loc': row['nearest_loc'], \n",
    "                'adm1': row['adm1'],\n",
    "                'adm2': row['adm2'], \n",
    "                'adm0': row['adm0'],\n",
    "                'idx': row.name}\n",
    "    \n",
    "    results = {'nearest_loc': False, 'idx': row.name}\n",
    "\n",
    "    point = Point((float(row[lon_col]), float(row[lat_col])))\n",
    "    nearest_record = None\n",
    "    nearest_distance = np.inf\n",
    "    for shape, country, adm1, adm2 in country_shapes(row[country_col]):\n",
    "        if point.within(shape):\n",
    "            results['adm0'] = country\n",
    "            results['adm2'] = adm2\n",
    "            results['adm1'] = adm1\n",
    "            break\n",
    "    else:\n",
    "        for shape, country, adm1, adm2 in country_shapes(row[country_col]):\n",
    "            distance = point.distance(shape)\n",
    "            if distance < nearest_distance:\n",
    "                nearest_distance = distance\n",
    "                nearest_record = (country, adm1, adm2)\n",
    "\n",
    "        if nearest_record:\n",
    "            results['nearest_loc'] = True\n",
    "            results['adm0'] = nearest_record[0]\n",
    "            results['adm1'] = nearest_record[1]\n",
    "            results['adm2'] = nearest_record[2]\n",
    "    \n",
    "    return results    \n",
    "            \n",
    "            \n",
    "locs[country] = 'no country'\n",
    "mask = locs['adm0'].isnull()\n",
    "pool_size = 30\n",
    "f = partial(match_single_geo, lon_col=lon, lat_col=lat, country_col=country)\n",
    "\n",
    "with Pool(pool_size) as p:\n",
    "    i = 0\n",
    "    for result in tqdm(p.imap_unordered(f, locs[mask].iterrows()), \n",
    "                       total=mask.sum()):\n",
    "        i += 1\n",
    "        idx = result['idx']\n",
    "        locs.loc[idx, 'adm2'] = result.get('adm2')\n",
    "        locs.loc[idx, 'adm1'] = result.get('adm1')\n",
    "        locs.loc[idx, 'adm0'] = result.get('adm0')\n",
    "        locs.loc[idx, 'nearest_loc'] = result['nearest_loc']\n",
    "        \n",
    "        if i % 10000 == 0:\n",
    "            locs.to_csv(output_folder / 'unique_locs_gpcc.csv', index=False)\n",
    "        \n",
    "locs.to_csv(output_folder / 'unique_locs_gpcc.csv', index=False)\n",
    "print('Total nearest:', locs['nearest_loc'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [11:53<00:00, 54.91s/it]\n"
     ]
    }
   ],
   "source": [
    "locs.drop(columns='country', inplace=True)\n",
    "for file in tqdm(glob('../Data/Precipitaciones/full_data_monthly_v2020_*.csv')):\n",
    "    file = Path(file)\n",
    "    df = pd.read_csv(file)\n",
    "    \n",
    "    df = df.merge(locs, on=['lat', 'lon'], how='left', validate='m:1')\n",
    "    df.to_csv(output_folder / file.name)\n",
    "    del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_time(df, from_, to_):\n",
    "    time = df['time'].str.split('-', expand=True)\n",
    "    time.columns = ['year', 'month']\n",
    "    \n",
    "    years = {str(year)[2:]: year for year in range(from_, to_+1)}\n",
    "    df['year'] = time['year'].map(years)\n",
    "    df['month'] = pd.to_numeric(time['month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_folder = Path('../Output/Precipitaciones/GPCC_preprocessing')\n",
    "time_groups = {'yearly': ['year'], 'monthly': ['year', 'month']}\n",
    "loc_groups = {'country': ['adm0'], \n",
    "              'edo': ['adm0', 'adm1'], \n",
    "              'mun': ['adm0', 'adm1', 'adm2']}\n",
    "\n",
    "groups = {}\n",
    "for loc_name, loc_group in loc_groups.items():\n",
    "    for time_name, time_group in time_groups.items():\n",
    "        groups[f'{loc_name}_{time_name}'] = loc_group + time_group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See : https://gis.stackexchange.com/questions/251812/returning-percentage-of-area-of-polygon-intersecting-another-polygon-using-shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ../Output/Precipitaciones/GPCC_preprocessing/full_data_monthly_v2020_1891_1900_025.csv\n",
      "Getting time\n",
      "Reading ../Output/Precipitaciones/GPCC_preprocessing/full_data_monthly_v2020_1901_1910_025.csv\n",
      "Getting time\n",
      "Reading ../Output/Precipitaciones/GPCC_preprocessing/full_data_monthly_v2020_1911_1920_025.csv\n",
      "Getting time\n",
      "Reading ../Output/Precipitaciones/GPCC_preprocessing/full_data_monthly_v2020_1921_1930_025.csv\n",
      "Getting time\n",
      "Reading ../Output/Precipitaciones/GPCC_preprocessing/full_data_monthly_v2020_1931_1940_025.csv\n",
      "Getting time\n",
      "Reading ../Output/Precipitaciones/GPCC_preprocessing/full_data_monthly_v2020_1941_1950_025.csv\n",
      "Getting time\n",
      "Reading ../Output/Precipitaciones/GPCC_preprocessing/full_data_monthly_v2020_1951_1960_025.csv\n",
      "Getting time\n",
      "Reading ../Output/Precipitaciones/GPCC_preprocessing/full_data_monthly_v2020_1961_1970_025.csv\n",
      "Getting time\n",
      "Reading ../Output/Precipitaciones/GPCC_preprocessing/full_data_monthly_v2020_1971_1980_025.csv\n",
      "Getting time\n",
      "Reading ../Output/Precipitaciones/GPCC_preprocessing/full_data_monthly_v2020_1981_1990_025.csv\n",
      "Getting time\n",
      "Reading ../Output/Precipitaciones/GPCC_preprocessing/full_data_monthly_v2020_1991_2000_025.csv\n",
      "Getting time\n",
      "Reading ../Output/Precipitaciones/GPCC_preprocessing/full_data_monthly_v2020_2001_2010_025.csv\n",
      "Getting time\n",
      "Reading ../Output/Precipitaciones/GPCC_preprocessing/full_data_monthly_v2020_2011_2019_025.csv\n",
      "Getting time\n"
     ]
    }
   ],
   "source": [
    "done = [Path(x).name for x in glob(str(preprocessing_folder / 'GPCC_*.csv'))]\n",
    "\n",
    "files = sorted(glob(str(preprocessing_folder / 'full_data_monthly_v2020_*.csv')))\n",
    "done = set(Path(x) for x in glob(str(preprocessing_folder / 'full_data_monthly_v2020_*.feather')))\n",
    "for i, file in enumerate(files):\n",
    "    feather = Path(file).with_suffix('.feather')\n",
    "    if feather in done:\n",
    "        continue \n",
    "        \n",
    "    print('Reading', file)\n",
    "    df = pd.read_csv(file)\n",
    "    df.drop(columns='Unnamed: 0', inplace=True)\n",
    "    df = df.melt(id_vars=['lon', 'lat', 'adm0', 'adm1', 'adm2', 'nearest_loc'],\n",
    "                 var_name='time')\n",
    "    \n",
    "    print('Getting time')\n",
    "    file_parts = file.split('_')\n",
    "    from_ = int(file_parts[-3])\n",
    "    to_ = int(file_parts[-2])\n",
    "    clean_time(df, from_, to_)\n",
    "    \n",
    "    df.to_feather(feather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "full_data_monthly_v2020_1891_1900_025.feather: 100%|█| 6/6 [02:50<00:00, 28.44s/\n",
      "full_data_monthly_v2020_1901_1910_025.feather: 100%|█| 6/6 [02:52<00:00, 28.75s/\n",
      "full_data_monthly_v2020_1911_1920_025.feather: 100%|█| 6/6 [02:58<00:00, 29.71s/\n",
      "full_data_monthly_v2020_1921_1930_025.feather: 100%|█| 6/6 [03:09<00:00, 31.55s/\n",
      "full_data_monthly_v2020_1931_1940_025.feather: 100%|█| 6/6 [02:50<00:00, 28.47s/\n",
      "full_data_monthly_v2020_1941_1950_025.feather: 100%|█| 6/6 [02:42<00:00, 27.01s/\n",
      "full_data_monthly_v2020_1951_1960_025.feather: 100%|█| 6/6 [02:41<00:00, 26.97s/\n",
      "full_data_monthly_v2020_1961_1970_025.feather: 100%|█| 6/6 [02:39<00:00, 26.57s/\n",
      "full_data_monthly_v2020_1971_1980_025.feather: 100%|█| 6/6 [02:40<00:00, 26.77s/\n",
      "full_data_monthly_v2020_1981_1990_025.feather: 100%|█| 6/6 [02:47<00:00, 27.97s/\n",
      "full_data_monthly_v2020_1991_2000_025.feather: 100%|█| 6/6 [02:53<00:00, 28.90s/\n",
      "full_data_monthly_v2020_2001_2010_025.feather: 100%|█| 6/6 [03:03<00:00, 30.53s/\n",
      "full_data_monthly_v2020_2011_2019_025.feather: 100%|█| 6/6 [03:01<00:00, 30.32s/\n"
     ]
    }
   ],
   "source": [
    "files = list(preprocessing_folder.glob('full_data_monthly_v2020_*.feather'))\n",
    "done = [x.name for x in preprocessing_folder.glob('GPCC_*.feather')]\n",
    "\n",
    "for i, file in enumerate(files):\n",
    "    df = pd.read_feather(file)\n",
    "    \n",
    "    for group_name, group in tqdm(groups.items(), desc=file.name):\n",
    "        fname = f'GPCC_{group_name}_b{i}.feather'\n",
    "#         if fname in done:\n",
    "#             continue\n",
    "            \n",
    "        n_groups = len(df.drop_duplicates(group))\n",
    "        collapsed = []\n",
    "\n",
    "        pivot = pd.pivot_table(df, \n",
    "                               values=['value', 'nearest_loc'],\n",
    "                               index=group,\n",
    "                               aggfunc={'value': [np.mean, np.median, np.std, 'count'], 'nearest_loc': np.mean})\n",
    "        pivot.columns = ['_'.join([level for level in reversed(columns) if level]) for columns in pivot.columns]\n",
    "        pivot.reset_index().to_feather(preprocessing_folder / fname)\n",
    "\n",
    "        pivot = pd.pivot_table(df[~df['nearest_loc']], \n",
    "                               values=['value'],\n",
    "                               index=group,\n",
    "                               aggfunc={'value': [np.mean, np.median, np.std, 'count']})\n",
    "        pivot['nearest_loc_mean'] = 0\n",
    "        pivot.columns = ['_'.join([level for level in reversed(columns) if level]) for columns in pivot.columns]\n",
    "        pivot.reset_index().to_feather(preprocessing_folder / f'GPCC_nnl_{group_name}_b{i}.feather')\n",
    "\n",
    "    del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPCC_nnl_country_yearly\n",
      "GPCC_country_yearly\n",
      "GPCC_nnl_country_monthly\n",
      "GPCC_country_monthly\n",
      "GPCC_nnl_edo_yearly\n",
      "GPCC_edo_yearly\n",
      "GPCC_nnl_edo_monthly\n",
      "GPCC_edo_monthly\n",
      "GPCC_nnl_mun_yearly\n",
      "GPCC_mun_yearly\n",
      "GPCC_nnl_mun_monthly\n",
      "GPCC_mun_monthly\n"
     ]
    }
   ],
   "source": [
    "files = list(preprocessing_folder.glob('GPCC*.feather'))\n",
    "output_folder = Path('../Output/Precipitaciones/GPCC')\n",
    "\n",
    "files_by_prefix = {}\n",
    "for file in files:\n",
    "    prefix = '_'.join(file.name.split('_')[:-1])\n",
    "    if prefix in files_by_prefix:\n",
    "        files_by_prefix[prefix].append(file)\n",
    "    else:\n",
    "        files_by_prefix[prefix] = [file]\n",
    "        \n",
    "for group_name, group in groups.items():\n",
    "    for pr in ('GPCC_nnl', 'GPCC'):\n",
    "        prefix = f'{pr}_{group_name}'\n",
    "        print(prefix)\n",
    "        files = files_by_prefix[prefix]\n",
    "\n",
    "        dfs = [pd.read_feather(file) for file in files]\n",
    "        df = dfs[0].append(dfs[1:], ignore_index=True)\n",
    "        del dfs\n",
    "\n",
    "        df = df.groupby(group).mean().reset_index()\n",
    "\n",
    "        df.to_csv(output_folder / (prefix + '.csv'), index=False)\n",
    "\n",
    "#         for file in files:\n",
    "#             os.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1891, 1892, 1893, 1894, 1895, 1896, 1897, 1898, 1899, 1900, 1901,\n",
       "       1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1910, 1911, 1912,\n",
       "       1913, 1914, 1915, 1916, 1917, 1918, 1919, 1920, 1921, 1922, 1923,\n",
       "       1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932, 1933, 1934,\n",
       "       1935, 1936, 1937, 1938, 1939, 1940, 1941, 1942, 1943, 1944, 1945,\n",
       "       1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1955, 1956,\n",
       "       1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967,\n",
       "       1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978,\n",
       "       1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989,\n",
       "       1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000,\n",
       "       2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011,\n",
       "       2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['year'].unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
