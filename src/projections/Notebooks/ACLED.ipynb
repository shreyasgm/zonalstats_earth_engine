{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "\n",
    "import shapefile\n",
    "import shapely\n",
    "from shapely.geometry import Point\n",
    "\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "import yaml\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "STOP = nlp.Defaults.stop_words.union(set(punctuation))\n",
    "\n",
    "pd.set_option('max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob('../Data/ACLED/*.xlsx')\n",
    "feather_file = Path('../Data/ACLED/acled.feather')\n",
    "totals_feather_file = Path('../Data/ACLED/acled_totals.feather')\n",
    "columns = None\n",
    "df = []\n",
    "totals = []\n",
    "\n",
    "if feather_file.exists() and totals_feather_file.exists():\n",
    "    df = pd.read_feather(feather_file)\n",
    "    totals = pd.read_feather(totals_feather_file)\n",
    "else:\n",
    "    for file in tqdm(files):\n",
    "        print(file)\n",
    "        continent = pd.read_excel(file)\n",
    "        continent.columns = [x.lower() for x in continent]\n",
    "\n",
    "        if columns is None:\n",
    "            columns = set(continent.columns)\n",
    "        elif 'Totals' not in file:\n",
    "            difference = columns.symmetric_difference(continent.columns)\n",
    "            assert len(difference) == 0, difference\n",
    "\n",
    "        continent['origin_filename'] = Path(file).name\n",
    "\n",
    "        if 'Totals' in file:\n",
    "            totals.append(continent)\n",
    "        else:\n",
    "            print('Appending')\n",
    "            df.append(continent)\n",
    "\n",
    "    df = df[0].append(df[1:], ignore_index=True)\n",
    "    totals = totals[0].append(totals[1:], ignore_index=True)\n",
    "\n",
    "    df.to_feather(feather_file)\n",
    "    totals.to_feather(totals_feather_file)\n",
    "    \n",
    "df.drop_duplicates(['event_id_cnty'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iso_codes = pd.read_csv('../Data/iso_codes.csv', usecols=['numeric', 'iso2'])\n",
    "iso_codes = iso_codes.append({'iso2': 'XK', 'numeric': 0}, ignore_index=True)\n",
    "\n",
    "# Namibia is interpreted as NaN\n",
    "iso_codes.loc[iso_codes['numeric'] == 516, 'iso2'] = 'NA'\n",
    "\n",
    "df = df.merge(iso_codes, left_on='iso', right_on='numeric', how='left')\n",
    "assert df['iso2'].isnull().sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shps = glob('../Shapefiles/preprocessed/*.shp')\n",
    "shapes_by_country = {}\n",
    "for i, shp in tqdm(enumerate(shps), total=len(shps)):\n",
    "    codes = Path(shp).name.replace('.shp', '').split('_')\n",
    "    shp = shapefile.Reader(shp)\n",
    "    shapes = [shapely.geometry.shape(s) for s in shp.shapes()]\n",
    "    records = shp.records()\n",
    "    \n",
    "    for code in codes:\n",
    "        shapes_by_country[code] = []\n",
    "        for shape, record in zip(shapes, records):\n",
    "            shapes_by_country[code].append((shape, *record))\n",
    "    \n",
    "assert 'GT' in shapes_by_country and 'PM' in shapes_by_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locs = df[['longitude', 'latitude', 'iso2']].drop_duplicates()\n",
    "locs['adm1'] = np.nan\n",
    "locs['adm2'] = np.nan\n",
    "locs['adm0'] = np.nan\n",
    "locs['nearest_loc'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Match locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def country_shapes(country):\n",
    "    if country in shapes_by_country:\n",
    "        yield from shapes_by_country[country]\n",
    "    else:\n",
    "        for shapes in shapes_by_country.values():\n",
    "            yield from shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = tqdm(total=locs.shape[0], desc='Finding codes')\n",
    "for idx, row in locs.iterrows():\n",
    "    if isinstance(row['adm1'], str):\n",
    "        pbar.update(1)\n",
    "        continue\n",
    "    \n",
    "    coordinates = (row['longitude'], row['latitude'])\n",
    "    point = Point(coordinates)\n",
    "    nearest_record = None\n",
    "    nearest_distance = np.inf\n",
    "    for shape, country, adm1, adm2 in country_shapes(row['iso2']):\n",
    "        if point.within(shape):\n",
    "            locs.loc[idx, 'adm0'] = country\n",
    "            locs.loc[idx, 'adm2'] = adm2\n",
    "            locs.loc[idx, 'adm1'] = adm1\n",
    "            break\n",
    "            \n",
    "        distance = point.distance(shape)\n",
    "        if distance < nearest_distance:\n",
    "            nearest_distance = distance\n",
    "            nearest_record = (shape, country, adm1, adm2)\n",
    "    else:\n",
    "        if nearest_record:\n",
    "            locs.loc[idx, 'nearest_loc'] = True\n",
    "            locs.loc[idx, 'adm2'] = nearest_record[3]\n",
    "            locs.loc[idx, 'adm1'] = nearest_record[2]\n",
    "            locs.loc[idx, 'adm0'] = nearest_record[1]\n",
    "        \n",
    "    pbar.update(1)\n",
    "pbar.close()\n",
    "print('Total nearest:', locs['nearest_loc'].sum())\n",
    "print('Missing:', \n",
    "      set(locs.loc[locs['adm1'].isnull(), 'iso2'].unique()).difference(locs.loc[locs['adm1'].notnull(), 'iso2'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locs.to_csv('../Output/ACLED/ACLED_locs.csv', index=False)\n",
    "# locs = pd.read_csv('../Output/ACLED/ACLED_locs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.merge(locs, on=['longitude', 'latitude', 'iso2'], how='left')\n",
    "assert df['adm0'].isnull().sum() == 0\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://reliefweb.int/sites/reliefweb.int/files/resources/ACLED_Codebook_2017FINAL%20%281%29.pdf\n",
    "# Page 26\n",
    "print(df.groupby('geo_precision')['geo_precision'].count())\n",
    "df.loc[df['geo_precision'] == 3, 'adm2'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../Output/ACLED/ACLED.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find keywords out of the field \"notes\" to use when collapsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Output/ACLED/ACLED.csv')\n",
    "# df.drop(columns=['adm0_name', 'adm1_name', 'adm2_name'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_TAGS = {'PROPN', 'ADJ', 'NOUN'}\n",
    "\n",
    "def get_hotwords(doc, is_doc=False):    \n",
    "    if not is_doc:\n",
    "        doc = nlp(doc)\n",
    "        \n",
    "    result = [token.lemma_ \n",
    "              for token in doc \n",
    "              if token.text.lower() not in STOP and token.pos_ in POS_TAGS]\n",
    "                \n",
    "    return result\n",
    "\n",
    "def get_hotwords_list(series):\n",
    "    words = []\n",
    "    for doc in tqdm(nlp.pipe(series, \n",
    "                        batch_size=10000, \n",
    "                        disable=['ner', 'senter', 'parser']),\n",
    "                   total=len(series)):\n",
    "        words.append(get_hotwords(doc, is_doc=True))\n",
    "    \n",
    "    return words\n",
    "\n",
    "def get_hotwords_set(series):\n",
    "    words = get_hotwords_list(series)    \n",
    "    return [list(set(w)) for w in words]\n",
    "\n",
    "def get_hotword_count(series, verbose=False):    \n",
    "    counter = Counter()\n",
    "    for doc in nlp.pipe(series, \n",
    "                        batch_size=5000, \n",
    "                        disable=['ner', 'senter', 'parser']):\n",
    "        words = get_hotwords(doc, is_doc=True)\n",
    "        counter.update(words)\n",
    "    \n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df['notes'].notnull()\n",
    "counter = get_hotword_count(df.loc[mask, 'notes'], verbose=True)    \n",
    "top50 = counter.most_common()[:50]\n",
    "    \n",
    "with open('top50_keywords_acled.yml', 'w') as f:\n",
    "    yaml.dump({k: v for k, v in top50}, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collapse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('top50_keywords_acled.yml') as f:\n",
    "    top50 = yaml.safe_load(f)\n",
    "    \n",
    "top50 = list(top50.keys()) + ['Venezuela', 'Venezuelan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_keywords():\n",
    "    with open('top50_keywords_scad.yml') as f:\n",
    "        top50 = yaml.safe_load(f)\n",
    "\n",
    "    top50 = list(top50.keys())\n",
    "    return top50\n",
    "\n",
    "\n",
    "def get_groups(loc_groups):\n",
    "    time_groups = {'yearly': ['year'], 'monthly': ['year', 'month'], 'weekly': ['year', 'week']}\n",
    "    additional = []\n",
    "\n",
    "    groups = {}\n",
    "    for loc_name, loc_group in loc_groups.items():\n",
    "        for time_name, time_group in time_groups.items():\n",
    "            groups[f'{loc_name}_{time_name}'] = loc_group + time_group + additional\n",
    "            \n",
    "    return groups\n",
    "\n",
    "\n",
    "def collapse(df, loc_groups, suffix=''):\n",
    "    if suffix and not suffix.startswith('_'):\n",
    "        suffix = '_' + suffix\n",
    "        \n",
    "    most_freqs = ['actor1', 'actor2', 'source']\n",
    "    additional = ['event_type', 'sub_event_type']\n",
    "    \n",
    "    top50 = load_keywords()\n",
    "    \n",
    "    df['event_date'] = pd.to_datetime(df['event_date'], format='%d-%b-%y')\n",
    "    df['year'] = df['event_date'].dt.year\n",
    "    df['month'] = df['event_date'].dt.month\n",
    "    df['week'] = df['event_date'].dt.isocalendar().week\n",
    "    \n",
    "    groups = get_groups(loc_groups)\n",
    "    \n",
    "    # Fill NAs\n",
    "    for group in groups.values():\n",
    "        for col in group:\n",
    "            df[col].fillna('Not available', inplace=True)\n",
    "            \n",
    "    done = [re.findall(r'ACLED_([^\\.]+)\\.csv', x)[0] for x in glob('../Output/ACLED/ACLED_*.csv')]\n",
    "    \n",
    "    for group_name, group in groups.items():\n",
    "        n_groups = len(df.drop_duplicates(group))\n",
    "        collapsed = []\n",
    "\n",
    "        if group_name in done:\n",
    "            print('Skipping', group_name)\n",
    "            continue\n",
    "\n",
    "        for key_values, subdf in tqdm(df.groupby(group), total=n_groups, desc=group_name):\n",
    "            counter = Counter()\n",
    "            for row in subdf['words']:\n",
    "                counter.update(row)\n",
    "\n",
    "            sub_group = {k: v for k, v in zip(group, key_values)}\n",
    "            sub_group['count'] = subdf.shape[0]\n",
    "            sub_group.update({f'keyword_{k}': counter.get(k, 0) for k in top50})\n",
    "            sub_group['fatalities'] = subdf['fatalities'].sum()\n",
    "\n",
    "            collapsed.append(sub_group)\n",
    "\n",
    "        collapsed = pd.DataFrame(collapsed)\n",
    "        collapsed.to_csv(f'../Output/ACLED/ACLED_{group_name}{suffix}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df['words'] = get_hotwords_set(df['notes'].fillna(''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collapse(df, loc_groups={'country': ['adm0'], \n",
    "                         'edo': ['adm0', 'adm1'], \n",
    "                         'mun': ['adm0', 'adm1', 'adm2']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethnic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Output/ACLED/ACLED.csv')\n",
    "df['iso3'] = df['adm0']\n",
    "df.drop(columns=['adm0', 'adm1', 'adm2', 'nearest_loc'], \n",
    "        inplace=True)\n",
    "\n",
    "locs = df[['iso3', 'latitude', 'longitude']].drop_duplicates()\n",
    "locs = gpd.GeoDataFrame(locs, geometry=gpd.points_from_xy(locs['longitude'], locs['latitude']))\n",
    "locs = locs.set_crs('EPSG:4326')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm = gpd.read_file('../Shapefiles/ethnic_preprocessed/tribe_adm0.shp')\n",
    "adm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find those that are within a shape\n",
    "locs = gpd.sjoin(adm, locs, how='right', op='contains')\n",
    "mask = locs['index_left'].isnull()\n",
    "print((~mask).sum(), 'exact matches')\n",
    "\n",
    "# To Flat CRS\n",
    "adm = adm.to_crs(epsg=3035)\n",
    "locs = locs.to_crs(epsg=3035)\n",
    "\n",
    "locs['nearest_loc'] = False\n",
    "cols = ['NAME', 'TRIBE_CODE']\n",
    "for idx, row in tqdm(locs[mask].iterrows(), total=mask.sum(), desc='Finding codes'):\n",
    "    argmin = adm.distance(row['geometry']).argmin()\n",
    "    match = adm.iloc[argmin]\n",
    "    \n",
    "    for col in cols:\n",
    "        locs.loc[idx, col] = match[col]\n",
    "    locs.loc[idx, 'nearest_loc'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_cols = ['iso3', 'NAME', 'TRIBE_CODE']\n",
    "df = df.merge(locs[['longitude', 'latitude'] + id_cols], \n",
    "              on=['longitude', 'latitude', 'iso3'], \n",
    "              how='left')\n",
    "df.to_csv('../Output/ACLED/ACLED_ethnic.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['words'] = get_hotwords_set(df['notes'].fillna(''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collapse(df, loc_groups={'ethnic': id_cols})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gen shapefiles codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = {}\n",
    "shps = ['../Shapefiles/GADM/gadm36_0.shp', '../Shapefiles/GADM/gadm36_1.shp', '../Shapefiles/GADM/gadm36_2.shp']\n",
    "for i, shp in enumerate(shps):\n",
    "    shp = shapefile.Reader(shp)\n",
    "    records = shp.records()\n",
    "    for record in records:\n",
    "        name = record[0]\n",
    "        r = {'adm0': record[0], \n",
    "             'adm0_name': record[1]}\n",
    "        \n",
    "        if i > 0:\n",
    "            r.update({'adm1': record[2],\n",
    "                      'adm1_name': record[3]})\n",
    "            name = record[2]\n",
    "            \n",
    "        if i == 2:\n",
    "            r.update({'adm2': record[5],\n",
    "                      'adm2_name': record[6]})\n",
    "            name = record[5]\n",
    "        codes[name] = r\n",
    "        \n",
    "print(len(codes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(codes.values()).to_csv('../Data/GADM_codes.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
