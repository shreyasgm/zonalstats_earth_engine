{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import re\n",
    "\n",
    "import shutil\n",
    "\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from projections import raster, utils\n",
    "\n",
    "\n",
    "pd.set_option('max_columns', None)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/sd2/Moncho/Protestas/.venv/lib/python3.8/site-packages/geopandas/_compat.py:106: UserWarning: The Shapely GEOS version (3.8.0-CAPI-1.13.1 ) is incompatible with the GEOS version PyGEOS was compiled with (3.9.1-CAPI-1.14.2). Conversions between both will be slow.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def save_location_mapping(row_and_path):\n",
    "    row, path = row_and_path\n",
    "    shape = row['geometry']\n",
    "    \n",
    "    subset = raster.find_subset_with_intersection_area(IMAGE, shape)\n",
    "\n",
    "    if subset.empty:\n",
    "        with open(path, 'w') as f:\n",
    "            f.write('')\n",
    "        return\n",
    "\n",
    "    subset['id'] = row['id']\n",
    "    \n",
    "    subset.to_csv(path, index=False)   \n",
    "    \n",
    "    \n",
    "def make_partial_path(parent):\n",
    "    path = output_path / parent / \"partial\"\n",
    "    path.mkdir(exist_ok=True, parents=True)\n",
    "    return path\n",
    "\n",
    "def make_countries_path(parent):\n",
    "    path = output_path / parent / \"by_country\"\n",
    "    path.mkdir(exist_ok=True, parents=True)\n",
    "    return path"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "read_path = Path('../Data/Population')\n",
    "output_path = Path('../Output/Population/')\n",
    "output_path.mkdir(exist_ok=True)\n",
    "\n",
    "# partial_path = output_path / 'partial'\n",
    "# by_country_path = output_path / 'by_country'\n",
    "# partial_path.mkdir(exist_ok=True)\n",
    "# by_country_path.mkdir(exist_ok=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Validate that all TIF share the same coordinates\n",
    "x = None\n",
    "y = None\n",
    "no_data_value = None\n",
    "\n",
    "all_tif = sorted(read_path.glob('**/*.tif'))\n",
    "for tif in all_tif:\n",
    "    image = utils.read_tif(tif)\n",
    "    if x is None:\n",
    "        x = image.x\n",
    "        y = image.y\n",
    "        no_data_value = image._FillValue\n",
    "    else:\n",
    "        assert np.all(x == image.x) and np.all(y == image.y), tif\n",
    "        assert no_data_value == image._FillValue"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "base_file = all_tif[0]\n",
    "partial_path = make_partial_path(base_file.parent.name)\n",
    "by_country_path = make_countries_path(base_file.parent.name)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Map raster to polygons"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "geo_df = gpd.read_file('../Shapefiles/preprocessed/all_countries_with_eth.shp')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "n_processes = 30\n",
    "\n",
    "IMAGE = utils.read_tif(base_file)\n",
    "\n",
    "iterator = partial(utils.yield_missing_shapes, save_path=partial_path, prefix=base_file.name[:-4])\n",
    "\n",
    "if n_processes == 1:\n",
    "    for row_and_path in tqdm(iterator(geo_df)):\n",
    "        save_location_mapping(row_and_path)\n",
    "else:\n",
    "    with ThreadPoolExecutor(n_processes) as tpe:\n",
    "        for _ in tqdm(\n",
    "            tpe.map(save_location_mapping, iterator(geo_df)), \n",
    "            total=geo_df.shape[0]\n",
    "        ):\n",
    "            pass"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 93%|██████████████████████████▊  | 113695/122772 [19:08:09<12:25:14,  4.93s/it]IOStream.flush timed out\n",
      " 93%|██████████████████████████▊  | 113696/122772 [19:08:32<16:32:53,  6.56s/it]IOStream.flush timed out\n",
      " 93%|██████████████████████████▊  | 113703/122772 [19:09:15<12:32:18,  4.98s/it]IOStream.flush timed out\n",
      " 93%|██████████████████████████▊  | 113704/122772 [19:09:42<27:21:24, 10.86s/it]IOStream.flush timed out\n",
      " 93%|██████████████████████████▊  | 113713/122772 [19:10:59<12:16:37,  4.88s/it]IOStream.flush timed out\n",
      " 93%|██████████████████████████▊  | 113716/122772 [19:11:56<33:55:09, 13.48s/it]IOStream.flush timed out\n",
      " 93%|██████████████████████████▉  | 114264/122772 [19:27:11<35:09:24, 14.88s/it]IOStream.flush timed out\n",
      " 93%|██████████████████████████▉  | 114265/122772 [19:27:25<40:03:10, 16.95s/it]IOStream.flush timed out\n",
      " 93%|██████████████████████████▉  | 114266/122772 [19:27:41<36:36:44, 15.50s/it]IOStream.flush timed out\n",
      " 93%|██████████████████████████▉  | 114271/122772 [19:28:35<28:40:40, 12.14s/it]IOStream.flush timed out\n",
      " 93%|██████████████████████████▉  | 114272/122772 [19:28:55<34:04:46, 14.43s/it]IOStream.flush timed out\n",
      " 93%|██████████████████████████▉  | 114279/122772 [19:31:12<40:39:16, 17.23s/it]IOStream.flush timed out\n",
      " 93%|██████████████████████████▉  | 114280/122772 [19:31:28<49:50:14, 21.13s/it]IOStream.flush timed out\n",
      " 93%|██████████████████████████▉  | 114285/122772 [19:32:52<37:09:50, 15.76s/it]IOStream.flush timed out\n",
      " 93%|██████████████████████████▉  | 114286/122772 [19:32:54<28:01:41, 11.89s/it]IOStream.flush timed out\n",
      " 93%|██████████████████████████▉  | 114289/122772 [19:33:26<24:36:19, 10.44s/it]IOStream.flush timed out\n",
      " 93%|██████████████████████████▉  | 114297/122772 [19:34:33<15:24:59,  6.55s/it]IOStream.flush timed out\n",
      " 93%|███████████████████████████  | 114306/122772 [19:36:00<19:57:15,  8.49s/it]IOStream.flush timed out\n",
      " 93%|███████████████████████████  | 114307/122772 [19:36:32<25:43:52, 10.94s/it]IOStream.flush timed out\n",
      " 93%|███████████████████████████  | 114308/122772 [19:36:53<44:03:07, 18.74s/it]IOStream.flush timed out\n",
      " 93%|███████████████████████████  | 114582/122772 [19:55:14<29:22:32, 12.91s/it]IOStream.flush timed out\n",
      " 93%|███████████████████████████  | 114583/122772 [19:55:27<30:07:01, 13.24s/it]IOStream.flush timed out\n",
      " 93%|████████████████████████████  | 114706/122772 [19:57:08<5:02:50,  2.25s/it]IOStream.flush timed out\n",
      " 93%|███████████████████████████  | 114709/122772 [19:57:27<10:47:50,  4.82s/it]IOStream.flush timed out\n",
      " 93%|███████████████████████████  | 114712/122772 [19:58:15<21:47:22,  9.73s/it]IOStream.flush timed out\n",
      " 93%|███████████████████████████  | 114715/122772 [19:59:05<29:53:47, 13.36s/it]IOStream.flush timed out\n",
      " 94%|███████████████████████████▏ | 115190/122772 [20:19:28<10:14:19,  4.86s/it]IOStream.flush timed out\n",
      " 94%|████████████████████████████▏ | 115412/122772 [20:32:36<6:53:48,  3.37s/it]IOStream.flush timed out\n",
      " 94%|████████████████████████████▏ | 115518/122772 [20:35:01<2:22:53,  1.18s/it]IOStream.flush timed out\n",
      " 94%|████████████████████████████▏ | 115521/122772 [20:35:15<4:16:39,  2.12s/it]IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      " 94%|████████████████████████████▏ | 115524/122772 [20:35:40<9:03:14,  4.50s/it]IOStream.flush timed out\n",
      " 94%|████████████████████████████▏ | 115525/122772 [20:35:45<7:33:28,  3.75s/it]IOStream.flush timed out\n",
      " 94%|███████████████████████████▎ | 115527/122772 [20:36:19<20:14:37, 10.06s/it]IOStream.flush timed out\n",
      " 94%|███████████████████████████▎ | 115669/122772 [20:38:54<11:40:44,  5.92s/it]IOStream.flush timed out\n",
      " 94%|████████████████████████████▎ | 115891/122772 [20:51:15<8:23:39,  4.39s/it]IOStream.flush timed out\n",
      " 94%|███████████████████████████▍ | 115986/122772 [20:53:32<12:11:37,  6.47s/it]IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      " 95%|████████████████████████████▍ | 116435/122772 [21:10:36<3:39:56,  2.08s/it]IOStream.flush timed out\n",
      " 95%|████████████████████████████▍ | 116438/122772 [21:11:01<6:05:53,  3.47s/it]IOStream.flush timed out\n",
      " 95%|████████████████████████████▌ | 116958/122772 [21:28:58<2:36:25,  1.61s/it]IOStream.flush timed out\n",
      " 95%|████████████████████████████▌ | 117061/122772 [21:31:34<7:12:21,  4.54s/it]IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      " 95%|███████████████████████████▋ | 117063/122772 [21:31:58<11:52:22,  7.49s/it]IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      " 96%|████████████████████████████▊ | 117968/122772 [22:05:30<4:32:41,  3.41s/it]IOStream.flush timed out\n",
      " 96%|███████████████████████████▉ | 118074/122772 [22:08:24<14:01:18, 10.74s/it]IOStream.flush timed out\n",
      " 97%|███████████████████████████▉ | 118485/122772 [22:25:05<12:30:24, 10.50s/it]IOStream.flush timed out\n",
      " 97%|████████████████████████████▉ | 118489/122772 [22:25:20<7:52:01,  6.61s/it]IOStream.flush timed out\n",
      " 97%|████████████████████████████▉ | 118655/122772 [22:28:02<1:02:36,  1.10it/s]IOStream.flush timed out\n",
      " 97%|█████████████████████████████ | 119013/122772 [22:44:49<3:56:58,  3.78s/it]IOStream.flush timed out\n",
      " 97%|█████████████████████████████ | 119017/122772 [22:45:37<7:17:06,  6.98s/it]IOStream.flush timed out\n",
      " 97%|█████████████████████████████ | 119018/122772 [22:46:17<8:59:13,  8.62s/it]IOStream.flush timed out\n",
      " 97%|████████████████████████████ | 119022/122772 [22:47:02<12:13:01, 11.73s/it]IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      " 97%|████████████████████████████ | 119023/122772 [22:47:55<20:18:39, 19.50s/it]IOStream.flush timed out\n",
      " 97%|████████████████████████████ | 119024/122772 [22:49:07<35:33:46, 34.16s/it]IOStream.flush timed out\n",
      " 97%|████████████████████████████ | 119034/122772 [22:51:34<14:31:22, 13.99s/it]IOStream.flush timed out\n",
      " 97%|█████████████████████████████ | 119053/122772 [22:52:58<4:49:35,  4.67s/it]IOStream.flush timed out\n",
      " 97%|████████████████████████████ | 119055/122772 [22:53:25<10:05:07,  9.77s/it]IOStream.flush timed out\n",
      " 97%|█████████████████████████████ | 119061/122772 [22:53:40<3:13:04,  3.12s/it]IOStream.flush timed out\n",
      " 97%|█████████████████████████████ | 119071/122772 [22:54:40<4:02:08,  3.93s/it]IOStream.flush timed out\n",
      " 97%|█████████████████████████████ | 119073/122772 [22:55:04<8:05:44,  7.88s/it]IOStream.flush timed out\n",
      " 97%|█████████████████████████████▏| 119443/122772 [23:08:20<3:04:22,  3.32s/it]IOStream.flush timed out\n",
      " 97%|█████████████████████████████▏| 119447/122772 [23:08:59<7:49:22,  8.47s/it]IOStream.flush timed out\n",
      " 97%|█████████████████████████████▏| 119451/122772 [23:09:14<5:31:50,  6.00s/it]IOStream.flush timed out\n",
      " 97%|████████████████████████████▏| 119461/122772 [23:11:01<15:44:56, 17.12s/it]IOStream.flush timed out\n",
      " 97%|████████████████████████████▏| 119463/122772 [23:11:28<13:49:29, 15.04s/it]IOStream.flush timed out\n",
      " 97%|█████████████████████████████▏| 119680/122772 [23:14:47<1:47:31,  2.09s/it]IOStream.flush timed out\n",
      " 98%|█████████████████████████████▎| 119878/122772 [23:30:26<8:12:15, 10.21s/it]IOStream.flush timed out\n",
      " 98%|█████████████████████████████▎| 119880/122772 [23:30:42<7:57:16,  9.90s/it]IOStream.flush timed out\n",
      " 98%|█████████████████████████████▎| 119881/122772 [23:31:08<9:24:13, 11.71s/it]IOStream.flush timed out\n",
      " 98%|█████████████████████████████▎| 120023/122772 [23:34:57<1:09:11,  1.51s/it]IOStream.flush timed out\n",
      " 98%|█████████████████████████████▎| 120029/122772 [23:35:49<3:58:33,  5.22s/it]IOStream.flush timed out\n",
      " 98%|████████████████████████████▍| 120282/122772 [23:47:36<10:19:57, 14.94s/it]IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      " 98%|█████████████████████████████▍| 120289/122772 [23:49:27<9:35:07, 13.90s/it]IOStream.flush timed out\n",
      " 98%|█████████████████████████████▍| 120292/122772 [23:49:59<7:12:50, 10.47s/it]IOStream.flush timed out\n",
      " 98%|█████████████████████████████▍| 120294/122772 [23:50:14<5:48:28,  8.44s/it]IOStream.flush timed out\n",
      " 98%|█████████████████████████████▍| 120296/122772 [23:50:47<8:10:45, 11.89s/it]IOStream.flush timed out\n",
      " 98%|█████████████████████████████▍| 120307/122772 [23:52:14<5:59:09,  8.74s/it]IOStream.flush timed out\n",
      " 98%|█████████████████████████████▍| 120308/122772 [23:52:22<5:42:36,  8.34s/it]IOStream.flush timed out\n",
      " 98%|████████████████████████████▍| 120310/122772 [23:53:04<10:03:06, 14.70s/it]IOStream.flush timed out\n",
      " 98%|████████████████████████████▍| 120311/122772 [23:53:51<16:48:30, 24.59s/it]IOStream.flush timed out\n",
      "100%|███████████████████████████████▉| 122585/122772 [24:08:59<02:12,  1.41it/s]\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Union portions from different files and shapes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "df_by_region = {}\n",
    "for file in tqdm(partial_path.glob('*.csv'), desc='Reading'):\n",
    "    try:\n",
    "        df = pd.read_csv(file)\n",
    "    except pd.errors.EmptyDataError:\n",
    "        continue\n",
    "        \n",
    "    if 'id' not in df.columns:\n",
    "        df['id'] = df['adm2']\n",
    "        df['id'].fillna(df['adm1'], inplace=True)\n",
    "        df['id'].fillna(df['adm0'], inplace=True)\n",
    "    region = df.loc[0, 'id']\n",
    "    df_by_region.setdefault(region, []).append(df)\n",
    "\n",
    "for region, dfs in tqdm(df_by_region.items(), desc='Saving'):\n",
    "    df = utils.combine_dataframes(dfs)\n",
    "    df.to_feather(by_country_path / f'{region}.feather')\n",
    "    \n",
    "del df_by_region"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Reading: 122772it [06:53, 296.96it/s]\n",
      "Saving: 100%|████████████████████████████| 48136/48136 [03:33<00:00, 225.43it/s]\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Aggregate all files with same raster"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def aggregate_one(file):\n",
    "    partial_path = make_partial_path(file.parent.name)\n",
    "    output_path = partial_path.parent / file.name\n",
    "    if output_path.exists():\n",
    "        return file.name\n",
    "    \n",
    "    file_path = partial_path.parent / file.name[:-4]\n",
    "    file_path.mkdir(exist_ok=True)\n",
    "\n",
    "    IMAGE = utils.read_tif(file)\n",
    "    increment = raster.get_increment_from_tif(IMAGE)\n",
    "    \n",
    "    for df_path in by_country_path.glob('*.feather'):\n",
    "        subdf_path = file_path / df_path.name\n",
    "        if subdf_path.exists():\n",
    "            continue\n",
    "            \n",
    "        df = pd.read_feather(df_path)\n",
    "        pol = utils.get_mock_polygon_from_df(df, increment=increment)\n",
    "        subdf = raster.merge_df_to_array_by_lat_lon(df, IMAGE, pol)\n",
    "        if subdf.empty:\n",
    "            print(df_path.name, 'is empty')\n",
    "        else:\n",
    "            subdf.to_feather(subdf_path)\n",
    "            \n",
    "    utils.aggregate_feather_splits_and_save(\n",
    "        input_path=file_path, \n",
    "        output_path=output_path, \n",
    "        no_data_value=no_data_value\n",
    "    )\n",
    "    shutil.rmtree(file_path)\n",
    "    return file.name\n",
    "    \n",
    "n_processes = min(len(all_tif), 1)\n",
    "print(f\"Running with {n_processes} processes\")\n",
    "if n_processes == 1:\n",
    "    for tif_file in all_tif:\n",
    "        print(aggregate_one(tif_file))\n",
    "else:\n",
    "    with ThreadPoolExecutor(n_processes) as tpe:\n",
    "        for name in tpe.map(aggregate_one, all_tif):\n",
    "            print(name)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Running with 1 processes\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Grouping:   0%|                             | 59/48136 [00:00<07:33, 106.13it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Error while reading ../Output/Population/Count/gpw_v4_population_count_rev11_2000_30_sec/AGO__YOMBE.feather. File is too small to be a well-formed file\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Grouping: 100%|██████████████████████████| 48136/48136 [06:50<00:00, 117.32it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "gpw_v4_population_count_rev11_2000_30_sec.tif\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Grouping:   0%|                              | 64/48136 [00:00<08:32, 93.72it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Error while reading ../Output/Population/Count/gpw_v4_population_count_rev11_2005_30_sec/BEN__EGBA.feather. File is too small to be a well-formed file\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Grouping: 100%|██████████████████████████| 48136/48136 [07:02<00:00, 113.80it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "gpw_v4_population_count_rev11_2005_30_sec.tif\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Grouping:   0%|                              | 59/48136 [00:00<08:24, 95.37it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Error while reading ../Output/Population/Count/gpw_v4_population_count_rev11_2010_30_sec/AGO__LUNDA.feather. File is too small to be a well-formed file\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Grouping: 100%|██████████████████████████| 48136/48136 [07:25<00:00, 108.17it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "gpw_v4_population_count_rev11_2010_30_sec.tif\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Grouping:   0%|                              | 63/48136 [00:00<08:06, 98.75it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Error while reading ../Output/Population/Count/gpw_v4_population_count_rev11_2015_30_sec/AGO__HOLO.feather. File is too small to be a well-formed file\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Grouping: 100%|██████████████████████████| 48136/48136 [07:13<00:00, 111.15it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "gpw_v4_population_count_rev11_2015_30_sec.tif\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Grouping:   0%|                              | 64/48136 [00:00<08:15, 97.06it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Error while reading ../Output/Population/Count/gpw_v4_population_count_rev11_2020_30_sec/AGO__YOMBE.feather. File is too small to be a well-formed file\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Grouping: 100%|██████████████████████████| 48136/48136 [07:22<00:00, 108.68it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "gpw_v4_population_count_rev11_2020_30_sec.tif\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Grouping: 100%|██████████████████████████| 48136/48136 [07:21<00:00, 109.13it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "gpw_v4_population_density_rev11_2000_30_sec.tif\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Grouping:   0%|                              | 61/48136 [00:00<08:39, 92.61it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Error while reading ../Output/Population/Density/gpw_v4_population_density_rev11_2005_30_sec/GHA__BRONG.feather. File is too small to be a well-formed file\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Grouping:  99%|██████████████████████████▋| 47509/48136 [06:48<00:18, 34.20it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Error while reading ../Output/Population/Density/gpw_v4_population_density_rev11_2005_30_sec/ATA.feather. Not an Arrow file\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Grouping: 100%|██████████████████████████| 48136/48136 [07:10<00:00, 111.82it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "gpw_v4_population_density_rev11_2005_30_sec.tif\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Grouping:   0%|                             | 50/48136 [00:00<07:49, 102.33it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Error while reading ../Output/Population/Density/gpw_v4_population_density_rev11_2010_30_sec/AGO__MBAGANI.feather. File is too small to be a well-formed file\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Grouping:  99%|██████████████████████████▋| 47510/48136 [06:56<00:17, 34.87it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Error while reading ../Output/Population/Density/gpw_v4_population_density_rev11_2010_30_sec/ATA.feather. Not an Arrow file\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Grouping: 100%|██████████████████████████| 48136/48136 [07:18<00:00, 109.74it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "gpw_v4_population_density_rev11_2010_30_sec.tif\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Grouping:   0%|                              | 61/48136 [00:00<08:32, 93.81it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Error while reading ../Output/Population/Density/gpw_v4_population_density_rev11_2015_30_sec/GHA__DAGOMBA.feather. File is too small to be a well-formed file\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Grouping: 100%|██████████████████████████| 48136/48136 [07:17<00:00, 109.94it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "gpw_v4_population_density_rev11_2015_30_sec.tif\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Grouping:   0%|                              | 57/48136 [00:00<08:18, 96.54it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Error while reading ../Output/Population/Density/gpw_v4_population_density_rev11_2020_30_sec/GHA__DAGOMBA.feather. File is too small to be a well-formed file\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Grouping: 100%|██████████████████████████| 48136/48136 [07:20<00:00, 109.35it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "gpw_v4_population_density_rev11_2020_30_sec.tif\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Join together results"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def read_population_df(file):\n",
    "    df = utils.robust_read(file)\n",
    "    year = get_year_from_population_file(file)\n",
    "    value_name = f'{file.parent.name}_{year}'\n",
    "    df.rename(columns={'value': value_name}, inplace=True)\n",
    "    return df\n",
    "\n",
    "def get_year_from_population_file(file):\n",
    "    return re.findall(\".*_(\\d{4})_.*\", file.name)[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "df = None\n",
    "\n",
    "for file in tqdm(output_path.glob('**/*.csv')):\n",
    "    if file.parent.name == 'partial':\n",
    "        continue\n",
    "        \n",
    "    field = read_population_df(file)\n",
    "    if field.empty:\n",
    "        continue\n",
    "    elif df is None:\n",
    "        df = field\n",
    "    else:\n",
    "        df = df.merge(field.drop(columns='intersection_area'), on='id', how='outer')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "122782it [00:02, 58513.95it/s] \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "df.to_csv(output_path / \"population.csv\", index=False)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}